{"cells": [{"cell_type": "markdown", "metadata": {"tags": []}, "source": ["### Machine Learning for Systems & Control 5SC28 2023-2024\n", "\n", "# Exercise set for Lecture 4 and 5: Gaussian Processes\n", "\n", "## Table of contents\n", "\n", "1. <a href=\"#Exercise-1:-Gaussian-Processes-for-dummies\">Exercise 1: Introduction to Gaussian Processes</a>\n", "2. <a href=\"#Exercise-2:-Choosing-a-good-prior-distribution-(i.e.-kernel-hyperparameter-optimization)\">Exercise 2: Choosing a good prior distribution (i.e. kernel hyperparameter optimization)</a>\n", "3. <a href=\"#Exercise-3:-Using-sklearn-GP\">Exercise 3: Using sklearn GP</a>\n", "4. <a href=\"#(demo)-Exercise-4:-Designing-custom-Kernels\">(demo) Exercise 4: Designing custom Kernels</a>\n", "5. <a href=\"#Exercise-5:-NARX-GP\">Exercise 5: NARX GP</a>\n", "6. <a href=\"#Exercise-6:-Bayesian-optimization\">Exercise 6: Bayesian optimization</a>\n", "6. <a href=\"#Exercise-7:-Sparse-Gaussian-Processes\">Exercise 7: Sparse Gaussian Processes</a>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In this exercise set, you will implement your own Gaussian Process-based estimator, explore its properties and how this approach can be succesfully deployed in practice. "]}, {"cell_type": "markdown", "metadata": {"tags": []}, "source": ["## Exercise 1: Intorduction to Gaussian Processes\n", "\n", "### Exercise 1.1: A GP with two points\n", "\n", "A univariate Gaussian distribution is given by\n", "\n", "$$p(y) = N(y|\\bar{y}, \\sigma_y) = \\frac{1}{\\sigma_y \\sqrt{2 \\pi}} \\exp \\left (-\\frac{(y-\\bar{y})^2}{2 \\sigma_y^2} \\right)$$\n", "\n", "where $\\bar{y}$ is the mean and $ \\sigma_y$ the stardard deviation of the distribution.\n", "\n", "This distribution can be generalized to multiple variables with as a [multivariate Gaussian distribution](https://en.wikipedia.org/wiki/Multivariate_normal_distribution). For instance, a multivariate Gaussian with two variables is given by \n", "\n", "$$\n", "p(y_1, y_2) = \\mathcal{N}(y| \\bar{y}, \\Sigma) =  (2 \\pi)^{-\\frac{2}{2}} \\det ( \\Sigma)^{-\\frac{1}{2}} \\exp \\left (-\\frac{1}{2} (y-\\bar{y})^\\top \\Sigma^{-1} (y-\\bar{y}) \\right) \\\\$$\n", "where \n", "$$ y = [y_1^\\top, y_2^\\top]^\\top \\\\\n", "\\bar{y} = [\\bar{y}_1^\\top, \\bar{y}_2^\\top]^\\top \\\\\n", "\\Sigma = \\begin{pmatrix}\n", "\\Sigma_{y_1, y_1} & \\Sigma_{y_1, y_2}\\\\ \n", "\\Sigma_{y_1, y_2} & \\Sigma_{y_2, y_2}\n", "\\end{pmatrix}\n", "$$\n", "\n", "with $\\Sigma$ the covariance matrix which is required to be a positive definite matrix (i.e. $(y-\\bar{y})^\\top \\Sigma (y-\\bar{y}) > 0$ for all $y \\neq \\bar{y}$)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**a)** Why is positive definiteness of $\\Sigma$ required for a multivariate gaussian distribution to be valid?\n", "\n", "\n", "*tip: what happens to the normalization integral? $\\int \\exp \\left (-\\frac{1}{2} (y-\\bar{y})^\\top \\Sigma^{-1} (y-\\bar{y}) \\right) dy$.*\n", "\n", "**Answer a):** fill by student\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**b)** $\\Sigma_{y_1,y_1}$ is the variance of the first variable. However, how can we interpret the off-diagonal terms $\\Sigma_{y_1,y_2}$? For this question use the cell below where a multi-variate Gaussian is sampled for $\\bar{y} = 0$ and \n", "$\n", "\\Sigma = \\begin{pmatrix}\n", "1& \\Sigma_{y_1, y_2}\\\\ \n", "\\Sigma_{y_1, y_2} & 1\n", "\\end{pmatrix}\n", "$ where you can set a list of off-diagonals $\\Sigma_{y_1, y_2}$ given by `offdias`. Note that $-1= -\\Sigma_{y_1,y_1} < \\Sigma_{y_1, y_2} < \\Sigma_{y_1,y_1} = 1$ is required for $\\Sigma$ to be positive definite.\n", "\n", "**Answer b):** fill by student\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "from matplotlib import pyplot as plt\n", "\n", "#give a list of off diagonals here\n", "offdias = # b) Fill this\n", "\n", "plt.figure(figsize=(4*len(offdias),4))\n", "for i,offdia in enumerate(offdias, start=1):\n", "    \n", "    mean_array = np.array([0,0])\n", "    covariance_matrix = np.array([[1,offdia],\n", "                                  [offdia, 1]])\n", "\n", "    y_samples = np.random.multivariate_normal(mean_array, covariance_matrix, size=1000)\n", "    #y_samples is a array of shape (N_samples, N_variables=2)\n", "    y1_samples = y_samples[:,0]\n", "    y2_samples = y_samples[:,1]\n", "\n", "    \n", "    plt.subplot(1,len(offdias),i)\n", "    plt.plot(y1_samples, y2_samples, '.')\n", "    plt.xlim(-4,4)\n", "    plt.grid()\n", "    plt.ylim(-4,4)\n", "    plt.title(f'off-diagonal={offdia:.2f}')\n", "    plt.xlabel('$y_1$')\n", "    plt.ylabel('$y_2$')\n", "plt.tight_layout()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["To better understand a Gaussian process, one can recognize that if $y_2$ is given then it will give information about $y_1$ if $\\Sigma_{y_1,y_2} \\neq 0$."]}, {"cell_type": "markdown", "metadata": {}, "source": ["**c)** To see the implications, derive $p(y_1| y_2)$ for $\\bar{y} = 0$ and $\\Sigma = \\begin{pmatrix}\n", "\\sigma & r\\\\ \n", "r & \\sigma\n", "\\end{pmatrix}$\n", "\n", "*Tip: use Bayes' Theorem ($p(y_1| y_2) p(y_2) = p(y_1,y_2)$) and Mathematica/Wolfram Alpha to compute the integrals you encounter in your calculations*\n", "\n", "*Tip: the matrices based joint distribution can be written as $$p(y_1, y_2) \\sim \\exp \\left(\\frac{-2 r^2 y_1 y_2+\\sigma ^2 y_1^2+\\sigma ^2 y_2^2}{2 r^4-2 \\sigma ^4}\\right)$$*\n", "\n", "**Answer c):** fill by student\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We call $p(y_1, y_2)$ the prior distribution and $p(y_1|y_2)$ the posterior distribution after it has been updated with measurement information $y_2$. \n", "\n", "\n", "This is an interesting result, since, this allows us to compute (refine) the probability distribution of an output given some observations. Next, we will generalize this results for a set of observations. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Exercise 1.2: A GP with many points\n", "\n", "The multi-variate Gaussian distribution is given by\n", "\n", "$$\n", "p(y) = \\mathcal{N}(y| \\bar{y}, \\Sigma)\\\\\n", "y = [y_1^\\top, y_2^\\top, y_3^\\top,...,y_N^\\top]^\\top \\\\\n", "\\bar{y} = [\\bar{y}_1^\\top, \\bar{y}_2^\\top, \\bar{y}_3^\\top,...,\\bar{y}_N^\\top]^\\top \\\\\n", "\\Sigma = \\begin{pmatrix}\n", "\\Sigma_{y_1, y_1} & \\Sigma_{y_1, y_2} & \\cdots & \\Sigma_{y_1, y_N} \\\\ \n", "\\Sigma_{y_2, y_1} & \\Sigma_{y_2, y_2} & \\cdots & \\Sigma_{y_2, y_N} \\\\\n", "\\vdots  & \\vdots  & \\ddots  & \\vdots  \\\\ \n", "\\Sigma_{y_N, y_1} & \\Sigma_{y_N, y_2} & \\cdots & \\Sigma_{y_N, y_N}\n", "\\end{pmatrix}\n", "$$\n", "\n", "However, if $y$ corresponds to observations of a single random variable, then this is not very useful as it treats every observation separately, needing $N$ means and $N(N+1)/2$ covariance matrix terms. To simplify this representation, assume that there is already some related information available. For example; if $y_i$ would be a temperature measurement then $x_i$ could be the time of day that the measurement was or will be made.\n", "\n", "Assume now that there exist functions which parameterize the mean and the covariance terms as;\n", "$$\\bar{y}_i = \\mu(x_i)\\\\\n", "\\Sigma_{y_i, y_j} = k(x_i, x_j)\n", "$$ \n", "\n", "which thus defines a prior distribution as\n", "\n", "$$p(Y) = \\mathcal{N} ( Y| \\mu(X), k(X, X))$$\n", "\n", "where\n", "\n", "$$\\mu(X) = [\\mu(x_1), \\mu(x_2), ..., \\mu(x_N)]^\\top$$\n", "\n", "$$k(X,X) = \\begin{pmatrix}\n", "k(x_1,x_1) & k(x_1,x_2) & \\cdots & k(x_1,x_N) \\\\ \n", "k(x_2,x_1)& k(x_2,x_2) & \\cdots & k(x_2,x_N) \\\\\n", "\\vdots  & \\vdots  & \\ddots  & \\vdots  \\\\ \n", "k(x_N,x_1) & k(x_N,x_2) & \\cdots & k(x_N,x_N)\n", "\\end{pmatrix}$$\n", "\n", "This does define a multi-variate Gaussian Process distribution as a prior. You will explore the properties of this prior in the next question. "]}, {"attachments": {"image.png": {"image/png": "iVBORw0KGgoAAAANSUhEUgAAAcIAAACTCAIAAACS3470AAAgAElEQVR4nO3dZ0AU19oH8DNbaUsHAemIEkB6EctVrGBDYwnRIEokURE1ppp7IxKTqDHRGFs0EI1eo8ZELLGF5IoFRYpLb6JIFRAW2GVh+7wfJuElILqyZRb2+X1IZs/OzvwVfHbmzMw5GI7jCAAAwEBRyA4AAACDG5RRAABQCJRRAABQCJRRAABQCJRRAABQCJRRAABQCJRRAABQCJRRAABQCJRRAABQCI3sAM8jk8nq6+tZLBaGYWRnAQBoLxzHeTyejY0NhfKsQ09cjW7cuDF79mxra2uEUEpKygvXr6mpUfffFgAA9KOmpuaZlUqtR6N8Pt/b23vFihULFiyQZ30Wi4UQqqmpMTQ0VHE0AADoF5fLtbOzIypSX2oto+Hh4eHh4fKvT5zLGxoaQhkFAyMQS1v4IhzH9Rk0Yz069A4BRfT3+6NxfaNCoVAoFBLLXC6X3DBgMGrvFF8ufHL7QXNOVWsDV9Ddrs+gvmJtGOJiFuZp5WFjRGJCMMRoXBndtm1bYmIi2SnAoFTRxNt//eGlgiciiay7kU7FKBgmlMj4Iml2VWt2Veve/1W4Wxu+PdF5tpcNlQLHp0BRGE7GeKMYhqWkpMybN6/vW72ORu3s7Nrb2+GkHjxfS4dw25XSX+/XEr/OblascE/rEBczV0sD4lxeIJbWtnber2q7Xtb0Z0mTSCpDCI2wNPh0rsfYEeYkpwcaj8vlGhkZ9VeLNO5olMlkMplMslOAweQcuy7hQlF7lxghNN19WFzoCC9bo17dWDp06ghL1ghL1uJAu7ZO0X8zqpJuV1Y0dSxJuveq7/AtER6GOnSS4oNBT+PKKADy6xRJ/pNSeJZdhxBytzb8bL6nn73JCz9lrMdYO9l12VjHr6+VHcuoOsuuy6ri7H3dz8fOWPWRwRCk1qeYOjo6cnNzc3NzEUKVlZW5ubnV1dXqDACGkrq2roUH755l11EwtGGq64W14+Spod0MdeiJEZ6/rBpra6Jbw+lafOhuCrtWdWnBEKbWvtG0tLTQ0NCeLdHR0UePHu1v/ef3RwBtVtrAfSMps7lDaG7A2L/EL9jZbMCb4grEG0/n/VHSiBDaMNV1/RRXuC8K9PL8WkTOJSY5QRkFz1RY1/5G8r22TrGbFSspOsDWRE/BDcpk+M7fyw6mPUQIRYc4JMzxoMAVfNDD82sRDE0CBpn71a2vf5/R1in2tjM+/VaI4jUUIUShYB+GuX0a4YFh6Me7VQkXijT58AJoGiijYDDJesyJSrrHE0gCHU3++2aQkZ4yL68vC3H8epE3hqHjGVXbr5RCJQVygjIKBo2yBl7M0Sy+SDrWxezHmCCWCm5RetXP9ov5oxFCh24+2vu/CqVvHwxJUEbB4FDf1hX9QyZPIAlwMPlheaAeQ1X36r0eZP/JbHeE0K7U8uTblSraCxhKoIyCQaC9S7z8SGYDV+BioZ8UHaBDp6p0d2+Od3p32kiE0GeXiq8VNah0X2AIeIkyKhaLa2pqysrKOByO6gIB0ItQIn3rWHZ5Y4cli/ljTJCxHkMNO107eUTUGAccRxtO5RbWtathj2DwenEZ7ejoOHTo0KRJk4yMjBwdHd3d3S0sLBwcHGJjY7OystQQEWgzHMe3XCi6V8lhMWlHVwQp5bq8PDAMS5jjPsHVvEssXfljdmOPkaIA6OUFZXT37t2Ojo7ff//95MmTz549m5ubW1ZWdvfu3YSEBIlEMm3atLCwsAcPHqgnK9BCJ+5Vn8yswTC0d4mvu41abx+mUSn7lviNsDRo4Apij2V3iaTq3DsYRF5w+/2iRYs2b948evToZ74rFAqTk5MZDMbKlStVEQ5uv9dymZWcJd9nSGT4h2Fuqye5kJKhuqUzYv/t1k7xq77Dv17sDQ84aSd4igkMSvVtXXP33W7uEM32st77ui+J9SvjUcvSpHtSGb51nmfUGAeyYgASKfMpph07diCE8vPzxWKxctIB8CwiiWz1ifvNHaJXrA2/XOhF7jHgGGezD8NGIYQ+vVh0v7qVxCRAM73czXfjx49HCG3ZsqWkpIROp3t6eo4ePXr06NGBgYHDhg1TTUKgjb68WppX02aoQzsc5a+6W0TlFzvBmV3ddqWwIe7E/d/ix5sZwJC44P8N/KS+s7OzsLCwoKCgoKDg1q1bM2fO3Lp1q3LDwUm9dvqjuHHlsWyE0KEo/xkeVmTH+QtPII7Yn/7oKX/cCLNjMcEw+4hWUU7fKI/H629yUYK/v39OTs4AM/YDyqgWqm/rmvntrbZO8YpxjglzPMiO8w/ljbyIfeldYum700bGT3ElOw5QH+X0jU6YMKGh4XmPc2RkZAwkHQA9iKWy+JPstk7x6OFGH4W7kR2nt5HDWFvneSKEdv9RnlkJD6GAv8hbRgMCAoKDg0tLS7tb2Gz2zJkzu1/S6TCVDVDUrtTynKpWFpO2f4kfk6baJz4HZqG/7au+w2U4WneSzeGLyI4DNIK8ZTQpKSkmJmb8+PG3b98uLy9fvHhxQEAAzD0HlCitrIkYOHn7Ai97MzU9rTQAW+d5OpvrN3AF75/J0+T7BYHavMQNTwkJCe++++60adM8PT27urqysrJSUlJUlwxolUauYOPPeQihN8bYz/KyJjvO8+gzaXuX+DJolD9Lm2AIKIDkL6NPnjxZt27d1q1b3d3d6XR6ZGSkn5+fSpMB7SGRyohzZHdrw//Mcic7zot52Bh9MusVhNCOq6X5tW1kxwEkk7eMOjs737p168yZMzk5OWfPnl2zZg1xKz4Aivv2zwf3Kjn6DOq+Jb6qHgRPWd4Y4xDuaSWW4mt/YnMF8DSKVpO3jB45coTNZs+aNQshNGPGjOvXr+/Zs2fNmjWqzAa0QnpF897rFQihL14d7WxhQHYceWEYtn2B13Bj3WpO56azBdBJqs3kLaORkZE9X/r5+d25cyctLU0FkYAWaeIJ1p/KxXEUGWgX4TOc7Dgvx0iXvneJL42CXcp/cjKzhuw4gDQDH/3e0dExPT1diVGAtpHK8HdO5zZ3CEcNY2nanfZy8rM3eX/GKIRQ4sWi0gYu2XEAORSaRMTExERZOYAW2ve/ivSKFl06df9SX13G4OgS7St2gvOkURZCiWztT+xOkYTsOIAEMBcTIMedh817/ixHCH02z3OE5fOeM9ZwFAr29SJvSxazoqlj8/kisuMAEkAZBSR4yhOuP5Urw9Eif9sF/rZkx1GUmQFzT6QvBUO/5NT+nA2dpFpHoTJKoVAmT56s9BFJwNBGdIk+5QldLQ0SIwZll2hfIS5m70wdiRD65FxhyRPoJNUuCpXRH374YeLEievWrVNWGqANDlyvuF3RrEunHljqpwljiSpLXOiIiSMthBJZ3In7PLiTVJvAJCJArTIetSz5PkOGo50LvRYF2JEdR8k4fNGsb289aRfM8rLeR+rEJ0C5lDmJyJ49exBCZWVlMplMOemANmniCtadZMtwtMDPdujVUISQqT5j3xI/4k7SY3eryI4D1OTlTqk8PT0RQu+8805FRYWBgYGHh4enp6enpyfxdBMAzyGSyNacuN/EE44cZrB13hDpEu3L38Fk08xXtv5W/NmlYm87Yx87Y7ITAZWT92iUx+MhhKZMmYIQunz5cnl5eVpa2urVq01MTFJTU1UYEAwVX1wuya5qZTFph6IChlKXaF8x4xzDPKzEUjzuxP2WDiHZcYDKyds36uPjc/XqVSsrtU6MA32jQ0YKu/ad03kIoaRlAVPdh/7sh1yBeO7e249bOsc4mx5/M5hOhTsLBzfl9I2+cPR7APpTWNe+6WwBQmjd5BHaUEMRQoY69O+XBegzqBmPOJ/9Vkx2HKBaMPo9UK0mriD2WLZALJs0ymL91JFkx1Ef12GsbyJ9EUI/3q06nVVNdhygQi/RRZWQkMBgMKZNmyaVSmfMmJGVlQUjN4Pn6xJJY49lP2kXuFjo74n01bZJiae5D3t32sivU8v/c67QxcIgwNGU7ERAJWD0e6AqMhn+7pncvNp2Ez36D8sDjXS1cdLDtZNHzBxtJZbisceyK5v5ZMcBKgGj3wNV2f1H+eWCBjoVOxQV4GCmT3YccmAY9tUib29bo9ZO8fIjmc1w4X4ogtHvgUocv/t47/8qEELbXvUKctLqk1k9Bi0pOtDOVLeqpfPNH7O7RFKyEwElg9HvgfJdyKvffKEIIbR+iuvCwT+Ak+IsWMyjK4KM9eh5NW3rTrElUngIcEh5QRmtru73CmP36Pd1dXVKDgUGs5vlT9/9ORfH0bIQhw1TXcmOoylcLAySlgUwaJTU4sb3f8mXyjR3LAvwsl5QRgMDA2NjYzMzM/u+1d7e/ssvv3h6ep49e1Y12cDgk1PFeft4jliKz/ay3jLHA8bm6CnA0XT/Ej8aBUth1/07pUAGlXSoeMENTyUlJV988UVYWBidTg8ICLCxsdHR0WltbS0uLi4qKgoICNi5c2d4eLh6sgINl1nJWXEks0ssneBqvmuxD0XLbm+SxzT3YXsifeNP3j+VVcOgURLnwjfNUCDXw6ACgeDy5cu3bt16/PhxV1eXubm5r6/vjBkziJFKVAceBh1E7j5siTma1SWWjnUxS4oe4k/NK+js/dp3z+ThOFo+1nHzbHf4vtF8z69F8j5T39jYOGyYuh/jgzI6WNx+0LzyWJZALJvgav79sgAd+mCdn05tTmZWEw/Ivuo3fMcCL3joXsMp55n6BQsWSCS9Zz3s2wK00IW8+pijWQKxLHSUBdRQOb0eZL9rsTeVgp29X7f6vzkCMdwFNYjJW0ZNTEzi4+N7trS0tEydOlUFkcCggeP4gbSKdSfZIqlshsew76L8oYbK71U/20Nv+DNplD9Kmpb9kNnKF5GdCAyQvGX0+PHjf/75Z1JSEvGypKQkKCgIzrW1mVgq+zil4MurZQihmHFOB5b6M2lQQ1/OVPdhx2KCWExaZiUnYn96WQOP7ERgIF5iLqaCgoKJEydeuXKltbU1MjLyrbfe2rFjh0qvM0LfqMZq5ArWnWTfq+RgGNo8233FOCeyEw1ipQ3c2GPZNZwuPQZ112LvME9rshOB3hS6xBQREeHj4+Pr6+vj4+Po6Hjy5Mn4+HiBQLB///7o6GiVZf4LlFHNdPtB8/pT7Ba+SJ9B3f2az3QPtQ7mPSS18kVxP92/87AFIbR6kss7U0cyaHDRSYModInJ1dU1PT09NjbW2dnZ1NT08OHDOI4vXbrUx8dHLIYpZLWOWCrb9XtZ1A/3WvgiNyvWxfjxUEOVwkSfcSwmKGacE0LoYNrDVw+mVzTBCf6gIe9JfW1tbW4PlZWVNBrNzc0tLy9PdeHgaFSj5Ne2ffBLfmkDDyH0epBdwhwPuKCkdFcKnmxKKWjrFDNplE3hbstCHOGuUk2gnPtGe+no6GCz2fn5+XFxcQon7BeUUQ3RKZLsTi1Pvl0pw5GJHj0xwnOutw3ZoYasRq7g/V/yb5Y/RQh52xolRnjC9KKkU0kZVQ8oo6STSGVncmp3p5Y38YQIoQgfm82z3c0MYPIY1cJx/HhG1ZdXyzqEEoTQawF274eNMoe/dvJAGQUDIZPhqSWNO6+VVTR1IIRsTXQ/jfCY7KYVE9JpiCauYPvV0rP36xBCegxq1BiHlROcLVhQTEkAZRS8HIFYevZ+XfLtRw+f8hFCJnr0tZNd3xhjD7eFkiKnipN4sTi/th0hpEOnLA12WD7W0c5Uj+xc2gXKKJDXg0ber/frzmTXtPBFCCEWk7ZsrMPbE10MdbRxGiXNgeN4WtnTb/58kFfThhDCMPQvV4ulwfaT3Sxp8DC+WkAZBS9Q3dKZWtJ4jl1XUNdOtAw31l0xzvG1QDsWFFCNgeP4jfKnybcrbz1oJlrMDRjTPaxmelqPcTaFeqpSUEbBM3AFYnZ1W3pF858ljcTJO0KIRsFC3SwX+A2f+sow+Gepsapa+D9lVp/JruX8/Ri+sR593AjzcS7m40aY2ZvqwRimSqdxZfTAgQM7d+588uSJh4fHN998M2HChP7WhDKqRJ0iSVkDr+QJr/hJe05VW2kDt/snT6VggY4mYR5Wc32Gm+ozSI0J5CWWyu4+bLlS+ORaUSOnx7AmVoY6XrZGXrZGo22NXS0NrAx14M5TxWlWGT19+nRUVNSBAwfGjRt36NChpKSk4uJie3v7Z64MZfRl4TjO7ZI084UtHaK6ts4aTlcNp7OmtbOG01Xf3tXrR21vqhfoaBrqZjHB1UI7J5EfGiRSGbumLb2i+U5FC7umVSz9x49Zh05xNNN3sTBwMtcfbqJryWJasnQsDZlm+gw44ZCfZpXR4OBgPz+/gwcPEi9feeWVefPmbdu27ZkrD6yM1rV14XjvPxbxEkf4P18Sy3839niru6Hnmv1u51mNvZbxHlvrtWu898q4RIpLZLhIKpNIcbFUJpbKxFJcLJVJpDKRFBdJZJ0iSYdQwhdK+EIpXyThCyU8gYTDF3H4Ikn/M/xYsJhuVqxXrA197IwDHEwsDXX6WxMMUp0iSWEdN7+2Lb+2vbC+vbqls7/fBwqGjPUYLB0aS4fGYtJZOjSWDp2lQzNg0hg0CoNGYVApxAKTRmHSKAwahUahUCkYhiEKhlEwjEpBGIZRMIzydwuF8tcChqG+B8B9uxqetU6flj5rydNj8cJ1TPQY+syXmKDh+bVIrTM9iESinJycjz76qLtl+vTpd+7c6bmOUCgUCoXEMpfLHcBeJu283usLWduwmDRTA4a1kY6diZ6dqZ69qZ6dqa6DmT7cvz3k6TFoQU6mQU6mxEuJVFbb2vWouePRU35lM7+hXdDEEzZyBc0dQhmOiO9dcgOT5cuFXosD7JS1NbWW0ebmZqlU2nMykmHDhjU0NPRcZ9u2bYmJiYrsRYdGpVJk6O/vse7vJeL/3V+JPf/X/dVFvPvMj/RY558bwHqu2f3B5+36Hx981keoFIxBpdCoGJ1KoVModBpGo1DoVAqdaKFS9JlUfSbNgEnTY/y1oM+kmekzTPUZpvoMeNQdEGhUiqO5vqO5/mS3f7RLZXgLX8jhizoEEp5AwhWIeQIJTyDhCcSdIqlQIhNJZEKJVCSRiaQykURGLIilOI7jMhyX4Ugm+3sBx2Wyvxf++i/ea9LT3gc1fQ5yejX0Opfse0zU+1zzn6vIc4JNVepVOBLmHetZTXAc71VcNm3atHHjRmKZy+Xa2b30N0ZB4gwFEwIwtFEpmCVLx5IFvTrKodYyam5uTqVSex5+NjU19Zopj8lkMpl/nXsSX0oDO7UHAABlIapQf1eS1FpGGQyGv79/amrq/PnziZbU1NSIiIj+1ufxeAihARyQAgCA0vF4PCMjo77t6j6p37hxY1RUVEBAQEhIyOHDh6urq1etWtXfyjY2NjU1NSwW66VuJya6Ampqaki8TQoyQAbIMJQy4DjO4/FsbJ49PqS6y+hrr73W0tLy6aefPnnyxNPT8/Llyw4ODv2tTKFQbG1tB7YjQ0ND0u82hQyQATIMmQzPPA4lkHCJac2aNWvWrFH/fgEAQBXgMQYAAFAIdcuWLWRnUD4qlTpp0iQajYRjbcgAGSCDtmXQ6BGeAABA88FJPQAAKATKKAAAKATKKAAAKATKKAAAKATKKAAAKGTol1GhUOjj44NhWG5urvr3PnfuXHt7ex0dHWtr66ioqPr6ejUHePz48Ztvvunk5KSrq+vi4pKQkCASkTDE5Oeffz527Fg9PT1jY2M17/rAgQNOTk46Ojr+/v63bt1S895v3rw5Z84cGxsbDMPOnTun5r0Ttm3bFhgYyGKxLC0t582bV1ZWpv4MBw8e9PLyIp4dCgkJuXLlivoz9LRt2zYMwzZs2KCUrQ39MvrBBx/09ySsGoSGhv78889lZWW//vrrw4cPFy5cqOYApaWlMpns0KFDRUVFu3fv/u677z7++GM1Z0AIiUSiRYsWrV69Ws37PX369IYNG/7973+z2ewJEyaEh4dXV1erMwCfz/f29t63b586d9rLjRs34uLiMjIyUlNTJRLJ9OnT+Xy+mjPY2tpu3749Ozs7Ozt78uTJERERRUVFas7QLSsr6/Dhw15eXkrbIj6kXb582c3NjfiBsdlscsOcP38ewzCRSERihi+//NLJyYmsvR85csTIyEidewwKClq1alX3Szc3t48++kidAbohhFJSUkjZdU9NTU0IoRs3bpAbw8TEJCkpiZRd83g8V1fX1NTUiRMnrl+/XinbHMpHo42NjbGxscePH9fT0yM7C+JwOCdOnBg7diydTubkce3t7aampiQGUCdi0prp06d3t/SdtEbbtLe3I4RI/B2QSqWnTp3i8/khISGkBIiLi5s1a9bUqVOVuM0hW0ZxHF++fPmqVasCAgLITfLhhx/q6+ubmZlVV1efP3+exCQPHz7cu3fvc0YmHGLkmbRGq+A4vnHjxvHjx3t6eqp/7wUFBQYGBkwmc9WqVSkpKe7u7urPcOrUqZycnP7m0BywQVxGt2zZgvUjOzt77969XC5306ZNJGYg1nn//ffZbPbvv/9OpVKXLVuGK/vpW3kyIITq6+vDwsIWLVq0cuVK5QaQPwMpnj9pjVZZu3Ztfn7+yZMnSdn7qFGjcnNzMzIyVq9eHR0dXVxcrOYANTU169evP3HihI6OkmdPGcTP1Dc3Nzc3Nz/zLUdHx8jIyIsXL3b/m5FKpVQqdenSpT/++KPaMvT6adXW1trZ2d25c0e5pzPyZKivrw8NDQ0ODj569CiFovzvTjn/Ho4ePbphw4a2tjalB3gmkUikp6d35syZ7tkW1q9fn5ube+PGDfUE6AnDsJSUlHnz5ql/14T4+Phz587dvHnTycmJrAzdpk6d6uLicujQIXXu9Ny5c/Pnz6dS/5rwUSqVYhhGoVCEQmF348CQOc6KgszNzc3Nzft799tvv/3ss8+I5fr6+hkzZpw+fTo4OFidGXohvrG6p49WW4a6urrQ0FB/f/8jR46ooobKk4EULztpzVCF43h8fHxKSkpaWpom1FCEEI7jSv+H8EJTpkwpKCjofrlixQo3N7cPP/xQwRqKBnUZfT57e/vuZQMDA4SQi4vLgMfSH5jMzMzMzMzx48ebmJg8evRo8+bNLi4uau5Zr6+vnzRpkr29/VdfffX06VOi0crKSp0ZEELV1dUcDqe6uloqlRI38I4YMYL4uajUS01aowodHR0VFRXEcmVlZW5urqmpac9fTjWIi4v76aefzp8/z2KxiK5hIyMjXV1ddWb4+OOPw8PD7ezseDzeqVOn0tLSrl69qs4ACCEWi9WzU5i4YqGcbmKlXO/XcJWVlYiMG57y8/NDQ0NNTU2ZTKajo+OqVatqa2vVnOHIkSOa8EOPjo7uleH69evq2fX+/fsdHBwYDIafn5/6b/S5fv16rz94dHS0mjP0/QU4cuSImjPExMQQPwULC4spU6b8/vvvag7QlxJveBrEfaMAAKAJBvGVegAA0ARQRgEAQCFQRgEAQCFQRgEAQCFQRgEAQCFQRgEAQCFQRgEAQCFQRgEAQCFQRgEAQCFQRoFGaGlpsbS0fPz4MYkZFi5cuGvXLhIDgEEKHgYFGuG9995rbW1NTk4mMQMxBkJlZaWhoWF34/Lly62srLZv305iMKDh4GgUkK+rqys5OVkV40m/FC8vL0dHxxMnTnS3yGSyS5cuaeHYeuClQBkF5Lty5QqNRuseQvDkyZM6Ojp1dXXEy5UrV3p5eRGTCA2YnNucO3duz8Hh09PTKRRK9zC1GRkZU6ZMMTc37zm8v9pGoQYaC8ooIN/Nmzd7TpkVGRk5atQoYsKcxMTEa9euXblyxcjISJFdyLnNoKCgzMzM7hGFL1y4MGfOHGKs67y8vEmTJnl7e9+8efPq1aumpqahoaGnT582NjZWJBgYCpQy3B4AioiIiIiJienZcvHiRSaT+fnnn5uYmBQWFhKNqampu3btGvBenrlNHMdv3br1xRdfEMt5eXkIocePHxMvR44ceeHCBWL5X//61+LFi7s/FRcXFxAQQCynpKQoa+RKMBhBGQXkmz59+po1a3o1+vr6MhiMtLS053wwISGhv+ODrKysvuu/cJvl5eUIoeLiYhzHi4uL9fT0Ojs7cRxvaGigUCg9P/jBBx8EBgYSy4mJicnJyXL/ccFQAyf1gHzm5uatra09W65du1ZaWtpreuTw8PCSkpKeq61du7akH30nh3jmNhFC8+fPJ+Y1QQhxOByEkIWFBULowoUL06ZNIybbyMnJkclk3t7e3Z/Kycnp7ojIz8/38vJS7O8ADGZk13EA8J07d3p7e3e/zMnJYbFYx44dmzlz5sKFC7vbXVxcxGLxwHbR3zZxHB81apRAICCWk5KSbG1tieWQkJDuY8yLFy8ihOrr64mX+fn5dDo9PT2deOnm5kYctALtBGUUkC8/P59Go3E4HBzHKysrraysPv/8cxzHs7OziZnucRxva2vrWWpfSn/bxHG8o6PDw8Oje83o6Giil7axsZFGozU2NhLtTU1Nurq6b7zxRklJyW+//ebk5BQfH0+8xefz3d3dBxYMDA1QRoFGGDNmzHfffdfS0uLm5vbWW291t8+dO3fGjBk4jt++fXvJkiUD2PJztonjeEZGxmuvvUYsd3V1GRoa3r17F8fxpKSkcePG9dzOxYsXR44cSafTnZ2dd+zYIZVKifZ79+4tWrRoAMHAkDFkJ1gGg8snn3zy3nvvxcbG9ur9PH/+PLFQWFg4sLlwTU1N+9smQqigoGD06NHEcnJycnBw8JgxY4h15s6d2/NTs2fPnj17dt/tQ8cogEtMQCPMnDnz7bff7r49vq+ioiLlTCn+Tz3LKJ1O37t3L7E8fvz4119/Xc4tQBnVcvBMPdBqfn5+ly5dsra2JjsIGMTgaBRoKZFI5OfnFxYWBjUUKAiORgEAQCFwNAoAAAqBMgoAAAqBMgoAAAqBMtB0W0kAAAA0SURBVAoAAAqBMgoAAAqBMgoAAAqBMgoAAAqBMgoAAAqBMgoAAAqBMgoAAAqBMgoAAAr5PzmFzaUagf7LAAAAAElFTkSuQmCC"}}, "cell_type": "markdown", "metadata": {}, "source": ["**d)** A very common parameterization of the kernel is the radial basis function kernel of the form \n", "$$\n", "k(x_i, x_j) = \\exp \\left ( \\frac{(x_i - x_j)^2}{2 \\sigma_l^2} \\right ).\n", "$$\n", "where $\\sigma_l$ is called the kernel width or scale length of the kernel. Implement this function `kernel_RBF` for a given array of points as $k(X,X)$. \n", "\n", "*tip: use `np.exp`*\n", "\n", "**e)** Compute the `covariance_matrix` and `mean_array` in the cell below and try out multiple kernel widths and mean functions. What is the general behaviour of the prior distributions and how does it depend on the kernel width and mean function? \n", "\n", "**Answer e):** fill by student\n", "\n", "**f)** How can this implication of the kernel width on the prior smoothness be explained based on the covariance interpretation of the off-diagonal terms?\n", "\n", "*Tip: use the kernel shape below*\n", "\n", "![image.png](attachment:image.png)\n", "\n", "*Tip: look back at Question b)*\n", "\n", "**Answer f):** fill by student\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def kernel_RBF(x1, x2, kernel_width):\n", "    #here x1 is a numpy array of different x values of shape (Nx1)\n", "    #here x2 is a numpy array of different x values of shape (Nx2)\n", "    # d) Fill this\n", "    Kx1x2 = # d) Fill this\n", "    return Kx1x2\n", "\n", "def mean_zero_function(x):\n", "    return np.zeros(x.shape[0])\n", "\n", "kernel_widths = # e) Fill this\n", "\n", "for kernel_width in kernel_widths:\n", "    print(f'kernel_width = {kernel_width}')\n", "    assert kernel_width>0\n", "    xtest = np.linspace(-3,3,num=300)\n", "    covariance_matrix = # e) Fill this\n", "    covariance_matrix = covariance_matrix + np.eye(len(xtest))*1e-10 #add small identity matrix to make it numerically well conditioned\n", "    mean_array = # e) Fill this\n", "\n", "    plt.figure(figsize=(8,2))\n", "    \n", "    Ysamples = np.random.multivariate_normal(mean=mean_array, cov=covariance_matrix, size=5)\n", "    for Ysample in Ysamples:\n", "        plt.plot(xtest,Ysample)\n", "    \n", "    plt.plot(xtest,mean_array,'k',label='mean')\n", "    plt.xlabel('$x$'); plt.ylabel('$y$')\n", "    plt.show()\n", "    plt.figure(figsize=(5,4))\n", "    plt.title(f'covariance matrix for {kernel_width}')\n", "    plt.contourf(xtest, xtest, covariance_matrix, origin='image')\n", "    plt.xlabel('$x_1$')\n", "    plt.ylabel('$x_2$')\n", "    plt.colorbar()\n", "    plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Like we did before in **c)** we can update the prior disitribution based on the given measurements. \n", "\n", "\n", "\n", "$$\n", "p\\left (\\left[\\begin{array}{l}\n", "Y_N \\\\\n", "Y_*\n", "\\end{array}\\right] \\right ) = \\mathcal{N}\\left(\\left[\\begin{array}{l}\n", "Y_N \\\\\n", "Y_*\n", "\\end{array}\\right] \\mid \\left[\\begin{array}{l}\n", "\\mu(X_N) \\\\\n", "\\mu(X_*)\n", "\\end{array}\\right],\\left[\\begin{array}{cc}\n", "k(X_N, X_N) & k(X_N, X_*) \\\\\n", "k(X_*, X_N) & k(X_*, X_*)\n", "\\end{array}\\right]\\right)$$\n", "\n", "\n", "\n", "$$\n", "p\\left (\\left[\\begin{array}{l}\n", "Y_N \\\\\n", "Y_*\n", "\\end{array}\\right] \\right ) = \\mathcal{N}\\left(\\left[\\begin{array}{l}\n", "Y_N \\\\\n", "Y_*\n", "\\end{array}\\right] \\mid \\left[\\begin{array}{l}\n", "\\mu_X \\\\\n", "\\mu_*\n", "\\end{array}\\right],\\left[\\begin{array}{cc}\n", "K_{XX}& K_{X*} \\\\\n", "K_{*X} & K_{**}\n", "\\end{array}\\right]\\right)$$\n", "\n", "$$\n", "p \\left ( Y_* \\mid Y_N \\right ) = \\mathcal{N}\n", "\\left (Y_* \\mid \\mu_*+K_{*X} K_{XX}^{-1} \\left(Y_N - \\mu_X \\right), K_{**}- K_{*X} K_{XX}^{-1} K_{X*} \\right )\n", "$$\n", "\n", "which uses the following well-known relation\n", "\n", "$$\n", "\\left[\\begin{array}{l}\n", "\\mathrm{v} \\\\\n", "\\mathrm{w}\n", "\\end{array}\\right] \\sim \\mathcal{N}\\left(\\left[\\begin{array}{l}\n", "\\mu_{\\mathrm{v}} \\\\\n", "\\mu_{\\mathrm{w}}\n", "\\end{array}\\right],\\left[\\begin{array}{cc}\n", "\\Sigma_{\\mathrm{vv}} & \\Sigma_{\\mathrm{vw}} \\\\\n", "\\Sigma_{\\mathrm{vw}}^{\\top} & \\Sigma_{\\mathrm{ww}}\n", "\\end{array}\\right]\\right)$$\n", "\n", "$$p(\\mathrm{w} \\mid \\mathrm{v})=\\frac{p(\\mathrm{v}, \\mathrm{w})}{p(\\mathrm{v})}=\\mathcal{N}\\left(\\mathrm{w}|\\mu_{\\mathrm{w}}+\\Sigma_{\\mathrm{vw}}^{\\top} \\Sigma_{\\mathrm{vv}}^{-1}\\left(\\mathrm{v}-\\mu_{\\mathrm{v}}\\right), \\Sigma_{\\mathrm{ww}}-\\Sigma_{\\mathrm{vw}}^{\\top} \\Sigma_{\\mathrm{vv}}^{-1} \\Sigma_{\\mathrm{vw}}\\right)$$\n", "\n", "With this, we are almost ready to implement our own Gaussian Process-based estimator. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Important note on mean (co-)variance and output (co-)variance and the white kernel\n", "\n", "Our outputs are often distrubed by noise processes. A very common manifestation of this phenomenon is white measurement noise, which can be defined as\n", "\n", "$$Y_N =  f_0(X_N) + E_N$$\n", "$$\n", "p(E_N) = \\mathcal{N}\\left( E_N| 0, I_N \\sigma_e^2 \\right )\n", "$$\n", "which modifies the output covariance as\n", "\n", "$$\n", "\\text{Cov}(Y_N, Y_N) = \\text{Cov}(f_0(X_N), f_0(X_N)) + \\text{Cov}(E_N, E_N)\\\\\n", " = \\text{Cov}(f_0(X_N), f_0(X_N)) + I_N \\sigma_e^2\\\\\n", "$$\n", "This additonal element introduced by the noise sometimes also called as a white kernel \n", "$$\n", "k_w(x_j, x_i) = \\sigma_e^2 \\delta_{ij}\n", "$$\n", "\n", "where $\\delta_{ij}=1$ if $i=j$ else $\\delta_{ij} = 0$. This means that we can disntinguish two GPs. One for the function $f_0$ charactersied by $\\text{Cov}(f_0(X_N), f_0(X_N))$ and one for the total output observation charactersied by $\\text{Cov}(f_0(X_N), f_0(X_N)) + I_N \\sigma_e^2$.\n", "\n", "Thus, for a combined kernel definition, if we want to obtain the covariance matrix of the GP (for the function) instead of the GP for the output, then we need to write\n", "\n", "$$\n", "p\\left (\\left[\\begin{array}{l}\n", "Y_N \\\\\n", "f_0(X_*)\n", "\\end{array}\\right] \\right ) = \\mathcal{N}\\left(\\left[\\begin{array}{l}\n", "Y_N \\\\\n", "f_0(X_*)\n", "\\end{array}\\right] \\mid \\left[\\begin{array}{l}\n", "\\mu_X \\\\\n", "\\mu_*\n", "\\end{array}\\right],\\left[\\begin{array}{cc}\n", "K_{XX}& K_{X*} \\\\\n", "K_{*X} & K_{**} - I_* \\sigma_e^2\n", "\\end{array}\\right]\\right)$$\n", "\n", "$$\n", "p \\left ( f_0(X_*) \\mid Y_N \\right ) = \\mathcal{N}\n", "\\left (f_0(X_*) \\mid \\mu_*+K_{*X} K_{XX}^{-1} \\left(Y_N - \\mu_X \\right), K_{**} - K_{*X} K_{XX}^{-1} K_{X*}  - I_* \\sigma_e^2 \\right )\n", "$$\n", "\n", "Thus we need to subtract $I_* \\sigma_e^2$ for the posterior covariance matrix to obtain the mean posterior covariance matrix.  \n", "\n", "\n", "Alternatively, the white kernel can be separated from the main kernel defining the GP for the function as\n", "\n", "$$\n", "p\\left (\\left[\\begin{array}{l}\n", "Y_N \\\\\n", "Y_*\n", "\\end{array}\\right] \\right ) = \\mathcal{N}\\left(\\left[\\begin{array}{l}\n", "Y_N \\\\\n", "Y_*\n", "\\end{array}\\right] \\mid \\left[\\begin{array}{l}\n", "\\mu_X \\\\\n", "\\mu_*\n", "\\end{array}\\right],\\left[\\begin{array}{cc}\n", "K_{XX}+ I_N \\sigma_e^2& K_{X*} \\\\\n", "K_{*X} & K_{**} + I_* \\sigma_e^2\n", "\\end{array}\\right]\\right)$$\n", "\n", "\n", "\n", "$$\n", "p\\left (\\left[\\begin{array}{l}\n", "Y_N \\\\\n", "f_0(X_*)\n", "\\end{array}\\right] \\right ) = \\mathcal{N}\\left(\\left[\\begin{array}{l}\n", "Y_N \\\\\n", "f_0(X_*)\n", "\\end{array}\\right] \\mid \\left[\\begin{array}{l}\n", "\\mu_X \\\\\n", "\\mu_*\n", "\\end{array}\\right],\\left[\\begin{array}{cc}\n", "K_{XX}+ I_* \\sigma_e^2& K_{X*} \\\\\n", "K_{*X} & K_{**} \n", "\\end{array}\\right]\\right)$$\n", "\n", "which is the commonly used form for GP-based estimators in practice."]}, {"cell_type": "markdown", "metadata": {}, "source": ["**g)** Below you can find a GP estimator based on the equations given above. Finish up this estimator by computing the posteriori mean and covariance. Furthermore, compute the standard deviation of the measurement $Y_*$ and the standard deviation of the mean $f_0(X_*)$ using the posterior covariance matrix. \n", "\n", "*Tip: use `np.linalg.inv` to compute the inverse, `np.diag` to extract the diagonal, `np.eye` to create an identity matrix.*\n", "\n", "*Tip: the standard deviation is the square root of the variance*\n", "\n", "*Note: a nummericaly efficient implementation can be achived by Cholesky factorization instead of really computing the inverse* "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def kernel_RBF(x1, x2, kernel_width):\n", "    x1_col = x1[:,None] #convert shape (Nx1) to (Nx1,1)\n", "    x2_row = x2[None,:] #convert shape (Nx2) to (1,Nx2)\n", "    diff_matrix = x1_col - x2_row # (Nx1,1) minus (1,Nx2) -> (Nx1, Nx2)\n", "    return np.exp(-diff_matrix**2/(2*kernel_width**2)) #return (Nx1, Nx2)\n", "\n", "def kernel_white(x1, x2, noise_level):\n", "    assert len(x1)==len(x2)\n", "    return np.eye(len(x1))*noise_level**2\n", "\n", "def kernel(x1, x2, kernel_width, noise_level, add_noise=True):\n", "    if add_noise:\n", "        return kernel_RBF(x1, x2, kernel_width) + kernel_white(x1, x2, noise_level)\n", "    else:\n", "        return kernel_RBF(x1, x2, kernel_width)\n", "\n", "def mean_zero_function(x):\n", "    return np.zeros(x.shape[0])\n", "\n", "\n", "## Generate data ##\n", "np.random.seed(23)\n", "N = 20\n", "xlim = 7\n", "measure_noise_std = 0.1\n", "X_N = np.random.uniform(-5.5,5.5, size=N)\n", "f0 = lambda x: x/4 + np.sin(x)\n", "Y_N = f0(X_N) + np.random.normal(scale=measure_noise_std, size=N)\n", "plt.figure(figsize=(12,3))\n", "plt.subplot(1,3,1)\n", "plt.title('measurement data')\n", "plt.ylabel('y')\n", "plt.xlabel('x')\n", "plt.plot(X_N, Y_N,'.')\n", "ylims = plt.ylim()\n", "\n", "#S = * in the script\n", "kernel_width = 0.9\n", "noise_level = measure_noise_std\n", "\n", "X_S = np.linspace(-xlim,xlim, 400) #X_*\n", "K_XX = kernel(X_N, X_N, kernel_width, noise_level, add_noise=True)  #K_XX\n", "K_SX = kernel(X_S, X_N, kernel_width, noise_level, add_noise=False) #K_*X\n", "K_XS = kernel(X_N, X_S, kernel_width, noise_level, add_noise=False) #K_*X\n", "K_SS = kernel(X_S, X_S, kernel_width, noise_level, add_noise=True) #K_*X\n", "mu_X = mean_zero_function(X_N)\n", "mu_S = mean_zero_function(X_S)\n", "\n", "K_XX_inv = # g) Fill this\n", "\n", "mu_post = # g) Fill this\n", "K_post  = # g) Fill this\n", "\n", "K_post_noiseless  = # g) Fill this\n", "\n", "#compute standard deviations:\n", "std_post = # g) Fill this\n", "std_mean_post = # g) Fill this\n", "\n", "#plot data\n", "plt.subplot(1,3,2)\n", "plt.xlabel('x')\n", "plt.plot(X_N, Y_N,'.r',label='data')\n", "plt.plot(X_S, f0(X_S),'g',label='$f_0$')\n", "plt.plot(X_S, mu_post,'k',label='post mean')\n", "plt.fill_between(X_S, mu_post-2*std_post,y2=mu_post+2*std_post,alpha=0.5,label='post std output')\n", "plt.fill_between(X_S, mu_post-2*std_mean_post,y2=mu_post+2*std_mean_post,alpha=0.5,label='post std function')\n", "plt.legend()\n", "plt.ylim(ylims)\n", "plt.xlim(-xlim,xlim)\n", "# plt.show()\n", "\n", "plt.subplot(1,3,3)\n", "plt.xlabel('x')\n", "Y_S_samples = np.random.multivariate_normal(mu_post, K_post_noiseless + np.eye(len(X_S))*1e-10, size=10)\n", "plt.plot(X_S, Y_S_samples.T,'b',alpha=0.5)\n", "plt.plot(X_N, Y_N,'.r')\n", "plt.plot(X_S, mu_post,'k')\n", "# plt.fill_between(X_S, mu_post-np.diag(K_post)**0.5,y2=mu_post+np.diag(K_post)**0.5,alpha=0.5)\n", "plt.ylim(ylims)\n", "plt.xlim(-xlim,xlim)\n", "plt.show()\n", "\n", "plt.figure(figsize=(12,3))\n", "plt.subplot(1,3,1)\n", "plt.ylabel('y')\n", "plt.xlabel('x')\n", "plt.title('mean')\n", "plt.plot(X_S, mu_S,label='mean prior')\n", "plt.plot(X_S, mu_post,label='mean posterior')\n", "plt.legend()\n", "\n", "to_img = lambda x: plt.contourf(X_S, X_S, x, origin='image', levels=np.linspace(np.min(x), np.max(x), 11))\n", "\n", "plt.subplot(1,3,2)\n", "plt.xlabel('$x_1$'); plt.ylabel('$x_2$')\n", "plt.title('prior $\\\\Sigma$')\n", "to_img(K_SS)\n", "plt.colorbar()\n", "plt.subplot(1,3,3)\n", "plt.xlabel('$x_1$'); plt.ylabel('$x_2$')\n", "plt.title('post $\\\\Sigma$')\n", "to_img(K_post)\n", "plt.colorbar()\n", "plt.tight_layout()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "### Theory note: A GP for functions\n", "\n", "The concept we discussed, can be extended to a probability distribution over all possible functions. like:\n", "$$\n", "p(f) = \\mathcal{N}(\\mu(\\cdot), k(\\cdot, \\cdot))\n", "$$\n", "This process can define a distribution of functions which can be updated with measurements like before. \n", "\n", "$$\n", "p\\left (\\left[\\begin{array}{l}\n", "Y_N \\\\\n", "f\n", "\\end{array}\\right] \\right ) = \\mathcal{N}\\left(\\left[\\begin{array}{l}\n", "Y_N \\\\\n", "f\n", "\\end{array}\\right] \\mid \\left[\\begin{array}{l}\n", "\\mu_X \\\\\n", "\\mu(\\cdot)\n", "\\end{array}\\right],\\left[\\begin{array}{cc}\n", "K_{XX}& k(X, \\cdot) \\\\\n", "k(\\cdot, X) & k(\\cdot, \\cdot)\n", "\\end{array}\\right]\\right)$$\n", "\n", "\n", "\n", "$$\n", "p \\left ( f \\mid Y_N \\right ) = \\mathcal{N}\n", "\\left (f \\mid \\mu(\\cdot)+k(\\cdot, X) K_{XX}^{-1} \\left(Y_N - \\mu(\\cdot) \\right), k(\\cdot, \\cdot)- k(\\cdot, X) K_{XX}^{-1} k(X, \\cdot) \\right )\n", "$$\n", "\n", "Hence, a Gaussian process takes a prior distribution of functions $p(f)$ and is able to update it with data to obtain a posterior distribution of functions $p(f|Y_N)$."]}, {"cell_type": "markdown", "metadata": {}, "source": ["###  Theory note: Relationship of GPs and Reproducing Kernel Hilbert Spaces\n", "\n", "GP is a specific version of a Reproducing Kernel Hilbert Spaces and the connected estimators which can model a vast range of (almost all) functions.\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### (optional) Exercise 1.3: How to define a GP if there is more than a single input?\n", "\n", "This is relatively simple since we can write the kernel for vectors as, for instance with an RBF kernel;\n", "\n", "$$\n", "k(x_i, x_j) = \\exp\\left ( - \\frac{\\| x_i - x_j\\|_2^2}{2 \\sigma_l} \\right )\n", "$$\n", "\n", "**h)** Implement the RBF kernel below for inputs which are vectors. Afterwards run the cell and observe the results for different kernel widths."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def kernel_RBF(x1, x2, kernel_width):\n", "    #x1 has shape (Nx1, nx)\n", "    #x2 has shape (Nx2, nx)\n", "    \n", "    # h) Fill this\n", "def kernel_white(x1, x2, noise_level):\n", "    assert len(x1)==len(x2)\n", "    return np.eye(len(x1))*noise_level**2\n", "\n", "def kernel(x1, x2, kernel_width, noise_level, add_noise=True):\n", "    if add_noise:\n", "        return kernel_RBF(x1, x2, kernel_width) + kernel_white(x1, x2, noise_level)\n", "    else:\n", "        return kernel_RBF(x1, x2, kernel_width)\n", "\n", "def mean_zero_function(x):\n", "    return np.zeros(x.shape[0])\n", "\n", "np.random.seed(23)\n", "N = 25\n", "X0_N = np.random.uniform(-5.,5., size=N)\n", "X1_N = np.random.uniform(-3.,3., size=N)\n", "X_N = np.array([X0_N, X1_N]).T\n", "\n", "measure_noise_std = 0.1\n", "\n", "f0 = lambda x: x[:,0]/4 + np.sin(x[:,1])*np.cos(x[:,0]) - (x[:,1]/3)**2\n", "Y_N = f0(X_N) + np.random.normal(scale=measure_noise_std, size=N)\n", "\n", "from matplotlib.cm import get_cmap\n", "cmap = get_cmap('viridis')\n", "plt.figure(figsize=(15,3))\n", "c = cmap((Y_N-np.min(Y_N))/(np.max(Y_N) - np.min(Y_N)))\n", "plt.subplot(1,4,1)\n", "plt.title('data (color = value)')\n", "plt.scatter(X0_N, X1_N, c=c)\n", "plt.xlabel('$x_1$'); plt.ylabel('$x_2$')\n", "\n", "X0_S = np.linspace(-5, 5, 25)\n", "X1_S = np.linspace(-3, 3, 20)\n", "X_S = np.stack(np.meshgrid(X0_S, X1_S), axis=-1).reshape(-1, 2)\n", "Y_S = f0(X_S)\n", "\n", "kernel_width = 1.85 \n", "\n", "noise_level = measure_noise_std\n", "\n", "K_XX = kernel(X_N, X_N, kernel_width, noise_level, add_noise=True)\n", "\n", "K_SX = kernel(X_S, X_N, kernel_width, noise_level, add_noise=False)\n", "K_XS = kernel(X_N, X_S, kernel_width, noise_level, add_noise=False)\n", "K_SS = kernel(X_S, X_S, kernel_width, noise_level, add_noise=True)\n", "mu_X = mean_zero_function(X_N)\n", "mu_S = mean_zero_function(X_S)\n", "\n", "K_XX_inv = np.linalg.inv(K_XX) \n", "\n", "K_post  = K_SS - K_SX@K_XX_inv@K_XS \n", "mu_post = mu_S + K_SX@K_XX_inv@(Y_N - mu_X) \n", "\n", "K_post_noiseless  = K_post - np.eye(len(X_S))*noise_level**2 \n", "\n", "std_post = np.diag(K_post)**0.5\n", "std_mean_post = np.diag(K_post_noiseless)**0.5\n", "\n", "plt.subplot(1,4,2)\n", "plt.xlabel('$x_1$'); plt.ylabel('$x_2$')\n", "plt.title('mean est by GP')\n", "l = plt.contourf(X0_S, X1_S, mu_post.reshape(X1_S.shape[0], X0_S.shape[0]))\n", "plt.subplot(1,4,3)\n", "plt.xlabel('$x_1$'); plt.ylabel('$x_2$')\n", "plt.title('mean real $f_0$')\n", "plt.contourf(X0_S, X1_S, Y_S.reshape(X1_S.shape[0], X0_S.shape[0]), levels=l.levels)\n", "\n", "plt.subplot(1,4,4)\n", "plt.xlabel('$x_1$'); plt.ylabel('$x_2$')\n", "plt.title('mean std of GP')\n", "plt.contourf(X0_S, X1_S, std_mean_post.reshape(X1_S.shape[0], X0_S.shape[0]))\n", "plt.scatter(X0_N, X1_N,c='r')\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Exercise 2: Choosing a good prior distribution (i.e. kernel hyperparameter optimization)\n", "\n", "With this exercise, we would like to address the issue of choosing good kernel hyperparameters. \n", "\n", "But fist, finish the implementation of the new GP posterior estimator.\n", "\n", "**a)** Finish `get_GP_post` which should return the mean function and the covarience function describing the posteriori predictive distirbution."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "from matplotlib import pyplot as plt\n", "\n", "def get_GP_post(kernel, sigma_e, X_N, Y_N, mean_function=None):\n", "    # kernel is a function like kernel([x1,x2,x3],[x1,x2,x3]) (without the white noise kernel)\n", "    # sigma_e is the noise amplitude\n", "    # X_N are the input points\n", "    \n", "    #evaluate kernel with white noise kernel\n", "    K_XX = # a) Fill this\n", "    K_XX_inv = # a) Fill this\n", "    if mean_function is None:\n", "        mu_X = np.zeros((X_N.shape[0],))\n", "    else:\n", "        mu_X = # a) Fill this\n", "\n", "    \n", "    #this function of the posterior will be returned and K_XX, K_XX_inv, mu_X can be used in function.\n", "    #context: https://www.geeksforgeeks.org/returning-a-function-from-a-function-python/\n", "    def evaluate_post(X_S, mean=False):\n", "        N_S = X_S.shape[0]\n", "        K_SX = # a) Fill this\n", "        K_XS = # a) Fill this\n", "        if mean==False:\n", "            K_SS = # a) Fill this\n", "        else:\n", "            K_SS = # a) Fill this\n", "\n", "        if mean_function==None:\n", "            mu_S = np.zeros((X_S.shape[0],))\n", "        else:\n", "            mu_S = # a) Fill this\n", "\n", "        mu_post = # a) Fill this\n", "        K_post  = # a) Fill this\n", "\n", "        return mu_post, K_post\n", "    \n", "    return evaluate_post\n", "\n", "def RBF_kernel(x1, x2, kernel_width):\n", "    diff = x1[:,None] - x2[None,:]\n", "    Kxx = np.exp(-diff**2/(2*kernel_width**2))\n", "    return Kxx"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "**b)** Read the data from `lownoise.mat` by placing it in the same folder as the notebook to obtain `X_N` and `Y_N`. We will estimate multiple GP on this data using different kernel parameters. This will be visualized in the `plot` function. Your task is to finish up the GP posterior estimation inside `plot` evaluated on a set of points `xvisualize`. This should also produce the mean, std, and std of the mean of the posterior for `xvisualize` such that it can be used in the visualization.\n", "Afterwards, run the cell and observe the dependence of the resutls on the chosen $\\sigma_e$ (assumed noise varaince) and the kernel width (assumed smoothness of the function). Explain this dependence on the hyper parameters and give a rough estimate of both $\\sigma_e$ and the kernel width!\n", "\n", "**Answer b):** fill by student\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from scipy.io import loadmat\n", "import numpy as np\n", "\n", "######### selecting dataset 'low', 'med' or 'high'  #########\n", "ident = 'high'\n", "\n", "#load data\n", "out = loadmat(f'{ident}noise.mat') #train data\n", "X_N = out['x'][:,0] #shape=(N,)\n", "Y_N = out['y'][:,0] #shape=(N,)\n", "out = loadmat(f'valid_{ident}noise.mat') #validation data\n", "xtest = out['x'][:,0] #shape=(Ntest,)\n", "ytest = out['y'][:,0] #shape=(Ntest,)\n", "\n", "xvisualize = np.linspace(-1.1,1.1,num=300) #xpoints for visualization\n", "\n", "\n", "def plot(X_N, Y_N, xvisualize, sigma_e = 0.1, kernel_width = 0.1):\n", "    \n", "    kernel = lambda x1, x2: RBF_kernel(x1, x2, kernel_width=kernel_width)\n", "    \n", "    #use get_GP_post:\n", "    GP_post = # b) Fill this\n", "\n", "    mu_visualize, K_visualize = # b) Fill this\n", "    std_visualize = # b) Fill this\n", "    _, K_visualize_mean = # b) Fill this\n", "    std_mean_visualize = # b) Fill this\n", "\n", "    plt.title(f'kernel width = {kernel_width}, $\\\\sigma_e = {sigma_e}$ ')\n", "    plt.plot(X_N,Y_N,'.',label='data')\n", "    plt.plot(xvisualize,mu_visualize,label='est mean')\n", "    plt.fill_between(xvisualize,\\\n", "                     mu_visualize-2*std_visualize,\\\n", "                     mu_visualize+2*std_visualize,alpha=0.3,label='est std output')\n", "    plt.fill_between(xvisualize,\\\n", "                     mu_visualize-2*std_mean_visualize,\\\n", "                     mu_visualize+2*std_mean_visualize,alpha=0.3,label='est std function')\n", "    plt.tight_layout()\n", "    plt.grid()\n", "    plt.xlim(min(xvisualize), max(xvisualize))\n", "    plt.ylim(-1.2, 1.2)\n", "\n", "\n", "plt.figure(figsize=(12,8))\n", "i = 0\n", "for sigma_e in [0.01, 0.07, 0.3 ]:\n", "    for kernel_width in [0.03,0.2,0.5,1.5]:\n", "        i+=1\n", "        plt.subplot(3,4,i)\n", "        plot(X_N, Y_N, xvisualize, sigma_e, kernel_width)\n", "        if i==1:\n", "            plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Having a rough estimate of the hyperparameters can be done for simple cases, but can be very difficult for a real-world problems. In the next questions, we will start exploring methods to automatically choose the hyperparameters. \n", "\n", "**c)** Split `X_N` and `Y_N` into a dedicated training of 80% and validation set of 20%. Afterwards, finish the implementation of the `RMS` function which estimates a GP posterior using `X_N_train`, `Y_N_train` and evaluates RMS on the validation data.\n", "\n", "*tip: RMS is given by $\\sqrt{\\frac{1}{N} \\sum (\\mu_i - y_i)^2}$*\n", "\n", "**d)** Run the cell below and observe the RMS landscape as a function of both parameters. It also visualizes the GP posterior of hyperparameters with the lowest RMS within this landscape. Do these hyperparameters seem realistic? And why is this the case when using the RMS validation method?\n", "\n", "**answer d):** The kernel width is quite realistic but $\\sigma_e$ is way too low. The error in $\\sigma_e$ is present since the RMS measure only considers the mean of the function which $\\sigma_e$ does not change significantly. \n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["val_frac = 0.2\n", "Ntrain = int(X_N.shape[0]*(1-val_frac))\n", "\n", "X_N_train, X_N_val = # c) Fill this\n", "Y_N_train, Y_N_val = # c) Fill this\n", "\n", "\n", "def RMS(X_N_train, Y_N_train, X_N_val, Y_N_val, sigma_e, kernel_width):\n", "    \n", "    kernel = lambda x1, x2: RBF_kernel(x1, x2, kernel_width=kernel_width)\n", "    GP_post = # c) Fill this\n", "    \n", "    mu_val, K_val = # c) Fill this\n", "    # c) Fill this\n", "sigma_e_list = np.geomspace(0.0001,3,num=21) #increasing value in log space\n", "kernel_width_list = np.geomspace(0.0001,2,num=22) #increasing value in log space\n", "\n", "mat_out = []\n", "for sigma_e in sigma_e_list:\n", "    mat_out_row = []\n", "    for kernel_width in kernel_width_list:\n", "        mat_out_row.append(RMS(X_N_train, Y_N_train, X_N_val, Y_N_val, sigma_e, kernel_width))\n", "    mat_out.append(mat_out_row)\n", "RMS_mat = np.array(mat_out)\n", "\n", "#plotting for d)\n", "plt.contour(kernel_width_list, sigma_e_list, np.clip(RMS_mat,-float('inf'),np.percentile(RMS_mat.flat,70)), levels=20)\n", "plt.loglog()\n", "plt.colorbar()\n", "plt.xlabel(r'kernel width')\n", "plt.ylabel(r'$\\sigma_e$')\n", "plt.title(\"RMS for both hyperparameters\")\n", "plt.show()\n", "\n", "\n", "best1, best2 = np.unravel_index(np.argmin(RMS_mat),RMS_mat.shape)\n", "plot(X_N, Y_N, xvisualize, sigma_e_list[best1],kernel_width_list[best2])\n", "plt.legend()\n", "plt.show()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["An alternative method is to minimise the marginal negative loglikelihood. This is possible since we can shape the prior distribution using kernel and mean parameters given by $\\eta$  as you saw it in Exercise 1.2. We should shape this prior such that it maximizes the probability $Y_N$ given $X_N$ is sampled from the prior distribution. In other words\n", "\n", "\n", "$$\n", "\\max_\\eta P(Y_N) = \\max_\\eta \\mathcal{N} ( Y_N| \\mu_\\eta(X_N), k_\\eta(X_N, X_N)) \\\\\n", "\\min_\\eta - \\log  P(Y_N) = \\min_\\eta - \\log \\mathcal{N} ( Y_N| \\mu_X, K_{XX}) \\\\\n", " =  \\min_\\eta  - \\log \\left ( (2 \\pi)^{-\\frac{N}{2}} \\det ( K_{XX})^{-\\frac{1}{2}} \\exp \\left (-\\frac{1}{2} (y-\\mu_X)^\\top K_{XX}^{-1} (y-\\mu_X) \\right) \\right )\\\\\n", "$$\n", "\n", "**e)** Finish the derivation by separating the log likelihood to a sum. \n", "\n", "**Answer e):** fill by student\n", "\n", "**f)** Do the same analysis as you did with the RMS validation set method but for minimization of the marginal negative loglikelihood. To do this analysis:\n", " 1. finish `eval_GP_prior` which returns $\\mu_X$ and $K_{XX}$ for the given kernel and mean function, \n", " 2. finish `meannegloglikelihood` which computes the mean (divided by `N`) negative log-likelihood given a mean and covariance matrix, \n", " 3. evaluate this method of picking the prior using the visualizations. \n", "\n", "**Answer f):** fill by student\n", "\n", "*Tip: use [slogdet](https://numpy.org/doc/stable/reference/generated/numpy.linalg.slogdet.html) to compute the log determinant efficiently*."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def eval_GP_prior(kernel, sigma_e, X_N, mean_function=None, mean = False):\n", "    # kernel is a function like kernel([x1,x2,x3],[x1,x2,x3]) (without the white noise kernel)\n", "    # sigma_e is the noise amplitude\n", "    # X_N are the input points\n", "    # mean_function is the mean function\n", "    # mean: bool which is true if the white noise kernel should not be applied.\n", "    \n", "    #returns the mean vector and the covariance matrix of the prior evaluated in X_N\n", "    if mean:\n", "        K_XX = # e) Fill this\n", "    else:\n", "        K_XX = # e) Fill this\n", "    if mean_function==None:\n", "        mu_X = np.zeros((X_N.shape[0],))\n", "    else:\n", "        mu_X = # e) Fill this\n", "    return mu_X, K_XX\n", "\n", "def meannegloglikelihood(Y_N, mu, K): #K of the standard deviation\n", "    N = len(Y_N)\n", "    norm_term = # e) Fill this\n", "    complexity_term = # e) Fill this\n", "    data_term = # e) Fill this\n", "\n", "    loglikelihood = -(norm_term + data_term + complexity_term)\n", "    return -loglikelihood/N\n", "\n", "def compute_meannegloglikelihood(X_N, Y_N, sigma_e, kernel_width):\n", "    kernel = lambda x1, x2: RBF_kernel(x1, x2, kernel_width=kernel_width)\n", "    mu_N, Kxx_N = eval_GP_prior(kernel=kernel, sigma_e=sigma_e, X_N=X_N) #no input!! this is the prior\n", "    return meannegloglikelihood(Y_N, mu_N, Kxx_N)\n", "    \n", "sigma_e_list = np.geomspace(0.01,1,num=40) #increasing value in log space\n", "kernel_width_list = np.geomspace(0.01,2,num=40) #increasing value in log space\n", "\n", "mat_out_like = [] \n", "for sigma_e in sigma_e_list:\n", "    mat_out_row = [] \n", "    for kernel_width in kernel_width_list: \n", "        mat_out_row.append(compute_meannegloglikelihood(X_N, Y_N, sigma_e, kernel_width))\n", "    mat_out_like.append(mat_out_row)\n", "mat_out_like = np.array(mat_out_like)\n", "\n", "#plotting for e)\n", "plt.contour(kernel_width_list, sigma_e_list, np.clip(mat_out_like,\\\n", "                                                     np.percentile(mat_out_like.flat,0),\\\n", "                                                     np.percentile(mat_out_like.flat,85)), levels=40)\n", "plt.colorbar()\n", "plt.xlabel(r'kernel width')\n", "plt.ylabel(r'$\\sigma_e$')\n", "plt.title(\"negative loglikelihood for both hyper parameters\")\n", "plt.loglog()\n", "plt.show()\n", "\n", "best1, best2 = np.unravel_index(np.argmin(mat_out_like),mat_out_like.shape)\n", "print('best kernel parameters:',kernel_width_list[best2], sigma_e_list[best1])\n", "print('min value:', np.min(mat_out_like))\n", "plot(X_N, Y_N, xvisualize, sigma_e_list[best1],kernel_width_list[best2])\n", "plt.legend()\n", "plt.show()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**g)** use `minimize` from `scipy.optimize` to find the maximum log likelihood."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from scipy.optimize import minimize\n", "#fun(x, *args) -> float (see docs)\n", "\n", "# maximize marginal log likelihood\n", "optimize_fun = lambda th: compute_meannegloglikelihood(X_N, Y_N, th[0], th[1])\n", "x0 = [sigma_e_list[best1],kernel_width_list[best2]] \n", "fsol = # g) Fill this\n", "\n", "sigma_e_best, kernel_width_best = fsol.x\n", "plot(X_N, Y_N, xvisualize, sigma_e_best, kernel_width_best)\n", "plt.legend()\n", "plt.show()\n", "\n", "print(f'best sigma_e {sigma_e_best:.3f}')\n", "print(f'best kernel width {kernel_width_best:.3f}')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**h)** re-run this exercise with 'med' and 'high' noise datasets (see *a)*). How does the optimal kernel width and $\\sigma_e$ change with the choice of dataset and why?\n", "\n", "**Answer h):** fill by student\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Exercise 3: Using sklearn GP\n", "\n", "Now that you understand the basics of Gaussian processes let's switch to a nicely implemented version included in sklearn [1.7 Gaussian Processes](https://scikit-learn.org/stable/modules/gaussian_process.html). It includes features like:\n", "\n", "* The hyperparameters of the kernel are optimized during the fitting of `GaussianProcessRegressor` by maximizing the log-marginal-likelihood (LML) (and using `scipy.optimize.minimize`). \n", "* Different kernels can be specified. Common kernels are provided, but it is also possible to specify custom kernels.\n", "\n", "Sklearn assumes that one uses a zero mean function. In case the mean function is fixed, this is not limiting since one can re-write \n", "\n", "$$\n", "p\\left (\\left[\\begin{array}{l}\n", "Y_N \\\\\n", "Y_*\n", "\\end{array}\\right] \\right ) = \\mathcal{N}\\left(\\left[\\begin{array}{l}\n", "Y_N \\\\\n", "Y_*\n", "\\end{array}\\right] \\mid \\left[\\begin{array}{l}\n", "\\mu(X_N) \\\\\n", "\\mu(X_*)\n", "\\end{array}\\right],\\left[\\begin{array}{cc}\n", "k(X_N, X_N) & k(X_N, X_*) \\\\\n", "k(X_*, X_N) & k(X_*, X_*)\n", "\\end{array}\\right]\\right)$$\n", "\n", "$$\n", "p\\left (\\left[\\begin{array}{l}\n", "Y_N - \\mu(X_N) \\\\\n", "Y_* - \\mu(X_*)\n", "\\end{array}\\right] \\right ) = \\mathcal{N}\\left(\\left[\\begin{array}{l}\n", "Y_N \\\\\n", "Y_*\n", "\\end{array}\\right] \\mid \\left[\\begin{array}{l}\n", "0 \\\\\n", "0\n", "\\end{array}\\right],\\left[\\begin{array}{cc}\n", "k(X_N, X_N) & k(X_N, X_*) \\\\\n", "k(X_*, X_N) & k(X_*, X_*)\n", "\\end{array}\\right]\\right)$$\n", "\n", "Thus it is equivalent to substracting the mean from the output data and adding the mean back to the prediction of the GP. However, if one wishes to parametrize the mean tune its hyper-parmaters together with the kernel then a custom made implementation is required.\n", "\n", "**a)** Construct a kernel as a combination of a Radial Basis Function `RBF` and a `WhiteKenel` using `+` and estimate a model using the x and y data generated below. Show the resulting model with `.predict` (set `return_std=True`).\n", "\n", "*Tip: read the documentation of Gaussian processes provided here [function doc](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html), [User guide](https://scikit-learn.org/stable/modules/gaussian_process.html#gaussian-process). Note that the width of the kernel is called scale length in this implementation.* \n", "\n", "*Tip: set `n_restarts_optimizer=10` for more robust hyperparamter optimization*\n", "\n", "*Note: sklearn uses multi-variate inputs so the x arrays need to have the shape of `(Nsamp, Nfeatures)` with `Nfeatures=1`*"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# f0 = lambda x: np.sin(3*x) + 0.5*np.random.normal(loc=0,scale=0.9,size=x.shape)\n", "import numpy as np\n", "from matplotlib import pyplot as plt\n", "np.random.seed(43)\n", "N = 100\n", "noise = 0.1\n", "f0 = lambda x: np.sin(3*x)\n", "\n", "x = np.random.normal(loc=0,scale=0.8,size=N)\n", "y = f0(x) + noise*np.random.normal(loc=0,scale=0.9,size=x.shape)\n", "xtest = np.linspace(-4,4,num=150)\n", "ytest = f0(xtest)\n", "\n", "plt.plot(x,y,'.')\n", "plt.grid(); plt.xlabel('x'); plt.xlabel('y'); plt.title('Data')\n", "plt.show()\n", "\n", "\n", "from sklearn.gaussian_process import GaussianProcessRegressor\n", "from sklearn.gaussian_process.kernels import RBF, WhiteKernel, ExpSineSquared\n", "\n", "\n", "#construct kernel and initial hyper-parameters\n", "ker = # a) Fill this\n", "#construct regressor\n", "reg = # a) Fill this\n", "#fit regressor and optimize hyperparameters using MAP\n", "reg.fit(x[:,None],y) \n", "\n", "print('Resulting kernel hyperparameters:',reg.kernel_) #print optimized kernel\n", "sigma_e = np.exp(reg.kernel_.k2.theta)**0.5 #extract the noise\n", "#use regressor\n", "ytest_p, ytest_std = # a) Fill this\n", "ytest_std_mean = (ytest_std**2 - sigma_e**2)**0.5 #remove sigma_e noise to get mean deviation\n", "\n", "#plot result\n", "plt.plot(xtest,ytest,'k',label='$f$')\n", "plt.plot(xtest,ytest_p,label='est mean')\n", "plt.xlim(min(xtest),max(xtest))\n", "plt.fill_between(xtest,ytest_p-2*ytest_std,ytest_p+2*ytest_std,alpha=0.3,label='est 2*std output')\n", "plt.fill_between(xtest,ytest_p-2*ytest_std_mean,ytest_p+2*ytest_std_mean,alpha=0.3,label='est 2*std function')\n", "plt.plot(x,y,'.',label='samples')\n", "plt.grid(); plt.legend(); plt.xlabel('x'); plt.ylabel('y'); plt.title('sklearn est')\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "Gaussian processes are probabilistic in nature, hence we can sample them. \n", "\n", "**c)** Sample 7 times the obtained Gaussian process on `xtest` using the mean and covariance matrix by calling `reg.predict(...,return_cov=True)`. Then, sample it using `np.random.multivariate_normal` and interpret the results.\n", "\n", "Alteratively these two operations are implemented by `ysamps = reg.sample_y(xtest[:,None],n_samples=7)`. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["ymean, ycov = # c) Fill this\n", "ysamps = # c) Fill this\n", "\n", "plt.plot(xtest,ysamps.T,alpha=0.7)\n", "plt.fill_between(xtest,ytest_p-2*ytest_std,ytest_p+2*ytest_std,alpha=0.3)\n", "plt.xlim(min(xtest),max(xtest))\n", "plt.grid(); plt.xlabel('x'); plt.ylabel('y'); plt.title('GP samples')\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**d)** Repeat the exercise trying out some other kernels provided in sklearn [Kernels](https://scikit-learn.org/stable/modules/gaussian_process.html#kernel-operators), for instance, the Exp-Sine-Squared kernel (`ExpSineSquared`)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["As seen in the implementation solving a GP requires solving a system of equations of $A x = y$ with a $A$ a shape of `(Nsamp,Nsamp)` which scales badly with an increasing data size. \n", "\n", "**e)** Measure the time it takes to estimate a model for different dataset sizes for `range(100,2500,200)` and save the time it takes to an array. Observe the scaling by plotting these saved times. (this computation should not take more than a minute)\n", "\n", "*Tip: use time.time() to get the current time in seconds from the time module*\n", "\n", "**f)** How does the computation time scale with the number of samples (linear or worse?)\n", "\n", "**Answer f):** fill by student\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.gaussian_process import GaussianProcessRegressor\n", "from sklearn.gaussian_process.kernels import RBF, WhiteKernel, ExpSineSquared\n", "import numpy as np\n", "def test(Nsamples):\n", "    N = Nsamples\n", "    noise = 0.1\n", "    f0 = lambda x: np.sin(3*x)\n", "\n", "    x = np.random.normal(loc=0,scale=0.8,size=N)\n", "    y = f0(x) + noise*np.random.normal(loc=0,scale=0.9,size=x.shape)\n", "\n", "    reg = GaussianProcessRegressor(RBF(length_scale=1) + WhiteKernel(noise_level=1.0)) \n", "    reg.fit(x[:,None],y) \n", "    return reg\n", "\n", "import time\n", "Nsamples_list = range(100,2500,200)\n", "# e) Fill this\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#plotting here\n", "# e) Fill this\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## (optional) Exercise 4: Designing custom Kernels\n", "\n", "One can also construct custom kernels using physical insight. For instance, if we know that the data has the following structure\n", "\n", "$y = a(x) x + e = f_0(x) + e$\n", "\n", "where $a(x)$ is a smooth function, we can construct the kernel by investigating the covariance between two points as follows:\n", "\n", "$$\\text{Cov}(y_i,y_j) = x_i  x_j + \\delta_{ij} \\sigma_e^2$$\n", "\n", "Since we know $a(x)$ is a smooth function we can now set $\\text{Cov}(a(x_i),a(x_j)) = k_{RBF} (x_i, x_j) x_j $ which gives the following GP prior:\n", "\n", "$$\n", "p\\left (\n", "Y_*\n", " \\right ) = \\mathcal{N}\\left(Y_* \\mid \n", "0\n", ",\\text{Cov}(Y,Y) \\right )\n", "$$\n", "\n", "$$\n", "p\\left (\n", "Y\n", " \\right ) = \\mathcal{N}\\left(Y \\mid \n", "0\n", ",X \\cdot k_\\text{RBF}(X,X) \\cdot X + I \\sigma_e^2 \\right )\n", "$$\n", "where\n", "$$\n", "[X \\cdot k_\\text{RBF}(X,X) \\cdot X]_{ij} = x_i k_{RBF} (x_i, x_j) x_j\n", "$$\n", "\n", "\n", "**a)** What are the main characterstics of these prior functions? Lastly, why are these characterstics expected given the kernel?\n", "\n", "**Answer a):** fill by student\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "from matplotlib import pyplot as plt\n", "\n", "def kernel_RBF(x1, x2, kernel_width = 1):\n", "    diff = x1[:,None] - x2[None,:]\n", "    return np.exp(- diff**2/(2*kernel_width))\n", "\n", "def kernelyy(x1, x2, kernel_width = 1):\n", "    A = kernel_RBF(x1, x2, kernel_width=kernel_width)\n", "    #or np.einsum('i,ij,j->ij', x1, A, x2)\n", "    return x1[:,None]*A*x2[None,:] \n", "\n", "noise_level = 1e-2 \n", "\n", "X = np.linspace(-5,5,100)\n", "kernel_width = 1\n", "K = kernelyy(X, X, kernel_width=kernel_width) + np.eye(len(X))*noise_level**2 \n", "mu = np.zeros(len(X))\n", "\n", "Y_samples = np.random.multivariate_normal(mu, K , size=10)\n", " \n", "plt.plot(X, Y_samples.T)\n", "plt.xlabel('$x$')\n", "plt.ylabel('$y$')\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Like always this is now a valid kernel and can be used as such\n", "\n", "\n", "$$\n", "p\\left (\\left[\\begin{array}{l}\n", "Y_N \\\\\n", "f_0(X_*)\n", "\\end{array}\\right] \\right ) = \\mathcal{N}\\left(\\left[\\begin{array}{l}\n", "Y_N \\\\\n", "f_0(X_*)\n", "\\end{array}\\right] \\mid \\left[\\begin{array}{l}\n", "0 \\\\\n", "0\n", "\\end{array}\\right],\\left[\\begin{array}{cc}\n", "K_{XX} + I_* \\sigma_e^2& K_{X X_*} \\\\\n", "K_{X_* X} & K_{X_* X_*} \n", "\\end{array}\\right]\\right)\n", "$$\n", "\n", "where we can compute \n", "$$\n", "p(f_0(X_*) | Y_N)\n", "$$\n", "with the conventional methods.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#data generation:\n", "X_N = np.random.uniform(low=-5, high=5, size = 100)\n", "areal = lambda X_N: np.sin(-(X_N**2)/(2*1.5**3))\n", "noise_level = 1e-1\n", "Y_N = areal(X_N)*X_N + np.random.normal(scale=noise_level, size=len(X_N))\n", "\n", "#X_*:\n", "x_test = np.linspace(-5, 5, 300)\n", "\n", "def meannegloglikelihood(Y_N, mu, K): #K of the standard deviation\n", "    N = len(Y_N)\n", "    norm_term =  N/2*np.log(2*np.pi) \n", "    complexity_term = 0.5*np.linalg.slogdet(K)[1] \n", "    data_term = 0.5*(Y_N-mu)@np.linalg.inv(K)@(Y_N-mu)\n", "\n", "    loglikelihood = -(norm_term + data_term + complexity_term)\n", "    return -loglikelihood/N\n", "\n", "#kernel_width optimization with a grid search:\n", "res = []\n", "kernel_widths = np.geomspace(1e-4, 1e4,)\n", "for kernel_width in kernel_widths:\n", "    K_YY = kernelyy(X_N, X_N, kernel_width=kernel_width) + np.eye(len(X_N))*noise_level**2\n", "    mu_YY = np.zeros(len(X_N))\n", "    res.append(meannegloglikelihood(Y_N, mu_YY, K_YY))\n", "kernel_width = kernel_widths[np.argmin(res)]\n", "print('kernel_width=',kernel_width)\n", "\n", "\n", "\n", "K_YY = kernelyy(X_N, X_N, kernel_width=kernel_width) + np.eye(len(X_N))*noise_level**2\n", "K_YS = kernelyy(X_N, x_test, kernel_width=kernel_width)\n", "K_SY = kernelyy(x_test, X_N, kernel_width=kernel_width)\n", "K_SS = kernelyy(x_test, x_test, kernel_width=kernel_width)\n", "\n", "mu_post = K_SY@np.linalg.inv(K_YY)@Y_N\n", "cov_mean_post = K_SS - K_SY@np.linalg.inv(K_YY)@K_YS\n", "std_mean_post = np.diag(cov_mean_post)**0.5\n", "std_post = std_mean_post + noise_level\n", "\n", "plt.plot(X_N, Y_N, 'r.')\n", "plt.plot(x_test, areal(x_test)*x_test ,'k')\n", "plt.plot(x_test, mu_post)\n", "plt.fill_between(x_test, mu_post - std_post*2, mu_post + std_post*2, alpha=0.5)\n", "plt.fill_between(x_test, mu_post - std_mean_post*2, mu_post + std_mean_post*2, alpha=0.5)\n", "plt.legend(['training data','real','est mean','est 2*std output','est 2*std function'])\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Something interesting that you can do with this kernel is that you can also obtain the distribution of $a(x)$. We can write out the following covariance:\n", "\n", "$$\\text{Cov}(y_i, a(x_j)) = \\text{Cov}(a(x_i) x_i + e_i, a(x_j))$$\n", "\n", "$$ = \\text{Cov}(a(x_i) , a(x_j)) x_i$$\n", "\n", "$$ = k_\\text{RBF}(x_i,x_j) x_i$$\n", "\n", "Hence we can write:\n", "\n", "$$\n", "p\\left (\\left[\\begin{array}{l}\n", "Y_N \\\\\n", "a(X_*)\n", "\\end{array}\\right] \\right ) = \\mathcal{N}\\left(\\left[\\begin{array}{l}\n", "Y_N \\\\\n", "a(X_*)\n", "\\end{array}\\right] \\mid \\left[\\begin{array}{l}\n", "0 \\\\\n", "0\n", "\\end{array}\\right],\\left[\\begin{array}{cc}\n", "\\text{Cov}(Y_N, Y_N) &  \\text{Cov}(Y_N, a(X_*))\\\\\n", "\\text{Cov}(Y_*, a(X_N)) & \\text{Cov}(a(X_*), a(X_*))\n", "\\end{array}\\right]\\right)\n", "$$\n", "\n", "$$\n", "p\\left (\\left[\\begin{array}{l}\n", "Y_N \\\\\n", "a(X_*)\n", "\\end{array}\\right] \\right ) = \\mathcal{N}\\left(\\left[\\begin{array}{l}\n", "Y_N \\\\\n", "a(X_*)\n", "\\end{array}\\right] \\mid \\left[\\begin{array}{l}\n", "0 \\\\\n", "0 \n", "\\end{array}\\right],\\left[\\begin{array}{cc}\n", "X_N \\cdot k_\\text{RBF}(X_N,X_N) \\cdot X_N + I_* \\sigma_e^2&  k_\\text{RBF}(X_N,X_*) \\cdot X_N\\\\\n", "X_* \\cdot k_\\text{RBF}(X_*,X_N) & k_\\text{RBF}(X_*,X_*)\n", "\\end{array}\\right]\\right)\n", "$$\n", "\n", "This means that you can compute  $p(a(X_*)|Y_N)$ using conventional methods.\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def kernelya(x1, x2, kernel_width = 1):\n", "    A = kernel_RBF(x1, x2, kernel_width=kernel_width)\n", "    return np.einsum('i,ij->ij', x1, A)\n", "\n", "def kernelay(x1, x2, kernel_width = 1):\n", "    A = kernel_RBF(x1, x2, kernel_width=kernel_width)\n", "    return np.einsum('ij,j->ij', A, x2)\n", "\n", "def kernelaa(x1, x2, kernel_width = 1):\n", "    return kernel_RBF(x1, x2, kernel_width=kernel_width)\n", "\n", "K_YY = kernelyy(X_N, X_N, kernel_width=kernel_width) + np.eye(len(X_N))*noise_level**2\n", "K_YA = kernelya(X_N, x_test, kernel_width=kernel_width)\n", "K_AY = kernelay(x_test, X_N, kernel_width=kernel_width)\n", "K_AA = kernelaa(x_test, x_test, kernel_width=kernel_width)\n", "\n", "mu_post = K_AY@np.linalg.inv(K_YY)@Y_N\n", "cov_post = K_AA - K_AY@np.linalg.inv(K_YY)@K_YA\n", "std_post = np.diag(cov_post)**0.5\n", "plt.plot(x_test, areal(x_test),'k')\n", "plt.plot(x_test, mu_post)\n", "plt.fill_between(x_test, mu_post - std_post*2, mu_post + std_post*2, alpha=0.5)\n", "plt.legend(['real','est mean a(x)','est std a(x)'])\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Exercise 5: NARX GP\n", "\n", "In the previous exercise, we explored how Gaussian Processes (GPs) can be used to estimate non-linear models. Let's apply GPs to the same example we used in the previous exercise set (Week 1).\n", "\n", "**a)** Construct the data arrays `Xtrain, Xval, Ytrain, Yval` using the cell below and estimate a GP with RBF and white kernel. Make a residual plot of both the training and validation data. Also include the uncertainty in the residual plot as a bar plot ([matplotlib errorbar plot](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.errorbar.html))\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "from matplotlib import pyplot as plt\n", "def f(upast,ypast):\n", "    ukm2, ukm1 = upast\n", "    ykm2, ykm1 = ypast\n", "    ystar = (0.8 - 0.5 * np.exp(-ykm1 ** 2)) * ykm1 - (0.3 + 0.9 * np.exp(-ykm1 ** 2)) * ykm2 \\\n", "           + ukm1 + 0.2 * ukm2 + 0.1 * ukm1 * ukm2\n", "    return ystar + np.random.normal(scale=0.01)\n", "\n", "def use_NARX_model_in_simulation(ulist, f, na, nb):\n", "    #init upast and ypast as lists.\n", "    upast = [0]*nb \n", "    ypast = [0]*na \n", "    \n", "    ylist = []\n", "    for unow in ulist:\n", "        #compute the current y given by f\n", "        ynow = f(upast,ypast) \n", "        \n", "        #update past arrays\n", "        upast.append(unow)\n", "        upast.pop(0)\n", "        ypast.append(ynow)\n", "        ypast.pop(0)\n", "        \n", "        #save result\n", "        ylist.append(ynow)\n", "    return np.array(ylist) #return result\n", "\n", "na, nb = 2, 2\n", "\n", "np.random.seed(42)\n", "N = 500\n", "ulist = np.random.normal(scale=1,size=N)\n", "ylist = use_NARX_model_in_simulation(ulist,f,na,nb)\n", "\n", "def make_training_data(ulist,ylist,na,nb):\n", "    #Xdata = (Nsamples,Nfeatures)\n", "    #Ydata = (Nsamples)\n", "    Xdata = []\n", "    Ydata = []\n", "    #for loop over the data:\n", "    for k in range(max(na,nb),len(ulist)): #skip the first few indexes such to \n", "        Xdata.append(np.concatenate([ulist[k-nb:k],ylist[k-na:k]])) \n", "        Ydata.append(ylist[k]) \n", "    return np.array(Xdata), np.array(Ydata)\n", "\n", "split = 0.75 #75% training and 25% validation split\n", "split_index = int(len(ulist)*split) \n", "Xtrain, Ytrain = make_training_data(ulist[:split_index],ylist[:split_index], na, nb) \n", "Xval,   Yval   = make_training_data(ulist[split_index:],ylist[split_index:], na, nb)\n", "print('Xtrain.shape',Xtrain.shape)\n", "print('Xval.shape',Xval.shape)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#fitting\n", "from sklearn.gaussian_process import GaussianProcessRegressor\n", "from sklearn.gaussian_process.kernels import RBF, WhiteKernel, ExpSineSquared\n", "\n", "ker = # a) Fill this\n", "reg = # a) Fill this\n", "# a) Fill this\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#residual calculations and plotting\n", "# a) Fill this\n", "plt.title('prediction on the training set')\n", "# a) Fill this\n", "plt.title('prediction on the validation set')\n", "# a) Fill this\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**b)** We are also interested in the simulation performance of the model. Make a simulation and plot the residual and the NRMS. Is it lower than the polynomial model of the last exercise set?\n", "\n", "**c)** Retry the exercise with different kernels and see if you can construct a kernel that is more accurate."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["np.random.seed(43)\n", "utest = np.random.normal(scale=1.0,size=5000)\n", "ytest = use_NARX_model_in_simulation(utest,f,na,nb)\n", "\n", "\n", "model_now = # b) Fill this\n", "fmodel = lambda u,y: model_now.predict(np.concatenate([u,y])[None,:])[0] \n", "ytest_sim = use_NARX_model_in_simulation(utest, fmodel, na, nb)\n", "# b) Fill this\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Exercise 6: Bayesian optimization\n", "\n", "\n", "In this exercise, we will explore the basics of Bayesian optimization. Consider the same setting as in Exercise 2, but now with a slightly modified problem setting as seen below. \n", "\n", "**a)** For a given set of 1-dimensional inputs and outputs estimate a GP regressor in `get_model` with an `RBF` kernel and a `WhiteKernel`. Also, write `get_mean_std` which returns the mean and the standard deviation for a given regressor and test points.\n", "\n", "**b)** Write the acquisition variance (`acquisition_var`) which takes in a number of test points and returns the estimated quality of picking each point based on the variance. \n", "\n", "**c)** Write the main `bayesian_optimization` function which in the first part samples `f` uniformly on the interval of `xmin` to `xmax` for `n_initial` points. Afterward, it should use the maximum of `acquisition_fun` (using `xtest_points`) to sample `f`  until having `n_max` points. Define the test set as `xtest = np.linspace(-3,3,num=1000)`. Visualize the results using the plotting already present below. \n", "\n", "*Tip: use np.argmax and np.append*\n", "\n", "**d)** Implement an acquisition function that weights the mean and the variance given by $\\mu(x) (1-w) + \\sigma(x) w$ which aims to find the maximum of the function while incorporating the variance of the function as well for exploration. Implement the `acquisition_weighted_mean_and_var` function and switching `have_d_been_implemented` to True. How does the behavior change for different values of $w$?\n", "\n", "There are many options for acquisition function, Further reading: https://distill.pub/2020/bayesian-optimization/\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import warnings\n", "from sklearn.gaussian_process import GaussianProcessRegressor\n", "from sklearn.gaussian_process.kernels import RBF, WhiteKernel, ExpSineSquared, ConstantKernel\n", "import numpy as np\n", "from matplotlib import pyplot as plt\n", "warnings.filterwarnings(\"ignore\") #This line suppresses warning messages during code execution.\n", "\n", "if True: #You can change the function to observe different behaviour.\n", "    f0 = lambda x: np.sin(3*x) + 0.2*x + 0.3 #without noise\n", "    xmin, xmax = -3., 3.\n", "else:\n", "    f0 = lambda x: x/20+x**2-x**4+np.exp(-(20*x)**2)/4\n", "    xmin, xmax = -1., 1.\n", "f = lambda x: f0(x) + np.random.normal(scale=0.005,size=np.array(x).shape) #noisy version\n", "\n", "def get_model(x,y): \n", "    #return a regressor which is fitted to x,y\n", "    ker = RBF(length_scale=0.4,length_scale_bounds=(0.01,5.0)) + \\\n", "          # a) Fill this\n", "def get_mean_std(gp_reg, xtest_points):\n", "    # a) Fill this\n", "    ytest_std_mean = (ytest_pred_std**2 - np.exp(gp_reg.kernel_.k2.theta))**0.5\n", "    return ytest_pred_mean, ytest_std_mean\n", "    \n", "def acquisition_var(gp_reg, xtest_points):\n", "    # b) Fill this\n", "def acquisition_weighted_mean_and_std(gp_reg, xtest_points, weight=0.5):\n", "    pass\n", "    # d) Fill this\n", "def bayesian_optimization(f, xmin, xmax, acquisition_fun, n_initial=5, n_max=15, seed=22):\n", "    # f : is the function which need to be sampled\n", "    # xmin : and xmax are the bounds on the x\n", "    # acquisition_fun(gp_reg, some_x_points) : is the acquisition_fun on which the maximum need to be chosen as next point\n", "    # n_initial : the number of points which are uniformly sampled from f before using bayesian optimizaiton\n", "    # n_max : the buget of the number of maximum points that can be sampled from f \n", "    # (i.e. n_initial - n_max is the number of bayesian samples)\n", "    \n", "    rng = np.random.RandomState(seed) #you can use rng as a random generator, (e.g. rng.uniform(xmin, xmax) will sample uniform)\n", "    x = # c) Fill this\n", "    y = # c) Fill this\n", "    xtest_points = # c) Fill this\n", "    # c) Fill this\n", "        gp_reg = get_model(x,y)\n", "        acquisition_vals = # c) Fill this\n", "        xnew = # c) Fill this\n", "        ynew = # c) Fill this\n", "        x = # c) Fill this\n", "        y = # c) Fill this\n", "    return x, y, get_model(x,y)\n", "\n", "n_initial = 5\n", "n_max = 15\n", "\n", "rng = np.random.RandomState(21)\n", "x_rand = rng.uniform(xmin, xmax, size=n_max) #random baseline\n", "y_rand = f(x_rand)\n", "x_test = np.linspace(xmin-0.05,xmax+0.05,num=5000)\n", "\n", "have_d_been_implemented = True  #switch to true when working on **d)**\n", "if have_d_been_implemented:\n", "    weight = 0.8\n", "    #incorporate the weight factor in the function with a lambda function\n", "    acquisition_weighted_mean_and_std_now = lambda gp_reg, xtest_points: \\\n", "        acquisition_weighted_mean_and_std(gp_reg, xtest_points, weight=weight)\n", "else:\n", "    acquisition_weighted_mean_and_var_now = None\n", "    \n", "for mode,acquisition_fun in enumerate([acquisition_var,acquisition_weighted_mean_and_std_now]):\n", "    if acquisition_fun==None:\n", "        continue\n", "    if mode==0:\n", "        print('Variance Acquision')\n", "    else:\n", "        print(f'Weighted mean and Variance Acquision (weight={weight})')\n", "    #Bayesian\n", "    x, y, reg = bayesian_optimization(f, xmin, xmax, acquisition_fun=acquisition_fun, n_initial=n_initial, n_max=n_max, seed=21)\n", "\n", "    plt.figure(figsize=(12,4))\n", "    for i,(xi, yi) in enumerate([(x_rand,y_rand),(x,y)]):\n", "        plt.subplot(1,2,i+1)\n", "        plt.plot(x_test,f0(x_test),label='real')\n", "\n", "        label = 'random samples' if i==0 else 'bayesian optimization'\n", "        plt.plot(xi,yi,'o',label=label)\n", "\n", "        reg = get_model(xi,yi)\n", "\n", "        ytest_pred_mean, ytest_pred_std_mean = get_mean_std(reg, x_test)\n", "        plt.plot(x_test, ytest_pred_mean,label='mean')\n", "        plt.fill_between(x_test, \\\n", "                         ytest_pred_mean+1.92*ytest_pred_std_mean,\\\n", "                         ytest_pred_mean-1.92*ytest_pred_std_mean,\\\n", "                         alpha=0.2,label='92% std function')\n", "        plt.grid()\n", "        plt.legend()\n", "        plt.ylabel('y')\n", "        plt.xlabel('x')\n", "    plt.show()\n", "    \n", "    if mode==1:\n", "        M = x_test[np.argmax(f0(x_test))]\n", "        x_near = np.linspace(M-0.05, M+0.05,num=300)\n", "        ytest_pred_mean, ytest_pred_std_mean = get_mean_std(reg, x_near)\n", "        \n", "        plt.plot(x_near, f0(x_near),label='real')\n", "        xlim, ylim = plt.xlim(), plt.ylim()\n", "        plt.plot(xi,yi,'o',label='bayesian samples')\n", "        plt.plot(x_near, ytest_pred_mean,label='mean')\n", "        plt.fill_between(x_near, \\\n", "                         ytest_pred_mean+1.92*ytest_pred_std_mean,\\\n", "                         ytest_pred_mean-1.92*ytest_pred_std_mean,\\\n", "                         alpha=0.2,label='92% std function')\n", "        plt.xlim(xlim); plt.ylim(ylim[0],ylim[1]+0.005)\n", "        plt.legend()\n", "        plt.grid()\n", "        plt.ylabel('y')\n", "        plt.xlabel('x')\n", "        plt.show()\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The grid-based optimization that we used above is not guaranteed to find the global\n", "optimum of the selection problem precisely, neither is it applicable in case of high-dimensional regression spaces.\n", "Hence, in case of a professional implementation a maximization over the acqusiation function is implemented as a nonlinear optimisation problem which can be solved by gradient-based techniques or by swarm optimization. Bayesian optimisation is a powerful method that can be deployed for highly complex problems and often used as a tool for selecting the hyperparameters for deep learning.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Exercise 7: Sparse Gaussian Processes\n", "\n", "This exercise is based on https://nbviewer.org/github/SheffieldML/notebook/blob/master/GPy/sparse_gp_regression.ipynb"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "**a)** install GPy by running `pip install --upgrade GPy`  https://gpy.readthedocs.io/en/deploy/index.html, https://github.com/SheffieldML/GPy\n", "\n", "If this fails you can retry this command in the anaconda command prompt and afterwards restart the notebook. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install --upgrade GPy\n", "\n", "# Note on Installing GPy:\n", "# If installing GPy using !pip install --upgrade GPy throws an error, follow these steps:\n", "# 1. Open Anaconda Prompt as an administrator.\n", "# 2. Activate the 'ml4sc' environment using the command 'conda activate ml4sc'.\n", "# 3. Re-run the command 'pip install --upgrade GPy'."]}, {"cell_type": "markdown", "metadata": {"tags": []}, "source": ["# Sparse GP Regression\n", "\n", "<!-- %### 14th January 2014 James Hensman\n", "%#### 29th September 2014 Neil Lawrence (added sub-titles, notes and some references). -->"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This example shows the variational compression effect of so-called 'sparse' Gaussian processes. In particular we show how using the variational free energy framework of [Titsias, 2009](http://jmlr.csail.mit.edu/proceedings/papers/v5/titsias09a/titsias09a.pdf) we can compress a Gaussian process without signifciantly worsening its prediction capabilites. First we set up the notebook with a fixed random seed, and import GPy."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["import GPy\n", "import numpy as np\n", "np.random.seed(101)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Sample Function\n", "\n", "Now we'll sample a Gaussian process regression problem directly from a Gaussian process prior. We'll use an exponentiated quadratic covariance function with a kernel width (length scale) of 1 and sample 50 equally spaced points. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["N = 50\n", "noise_var = 0.05\n", "\n", "X = np.linspace(0,10,50)[:,None]\n", "k = GPy.kern.RBF(1)\n", "y = np.random.multivariate_normal(np.zeros(N),k.K(X)+np.eye(N)*np.sqrt(noise_var)).reshape(-1,1)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Full Gaussian Process Fit\n", "\n", "Now we use GPy to optimize the parameters of a Gaussian process given the sampled data. Here, there are no approximations, we simply fit the full Gaussian process."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["m_full = GPy.models.GPRegression(X,y)\n", "m_full.optimize('bfgs')\n", "m_full.plot()\n", "print (m_full)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## A Poor `Sparse' GP Fit\n", "\n", "Now we construct a sparse Gaussian process. This model uses the inducing variable approximation and initialises the inducing variables in two 'clumps'. Our initial fit uses the *correct* covariance function parameters, but a badly placed set of inducing points. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Z = np.hstack((np.linspace(2.5,4.,3),np.linspace(7,8.5,3)))[:,None]\n", "m = GPy.models.SparseGPRegression(X,y,Z=Z)\n", "m.likelihood.variance = noise_var\n", "m.plot()\n", "print (m)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "Notice how the fit is reasonable where there are inducing points, but bad elsewhere. \n", "\n", "### Optimizing Covariance Parameters\n", "\n", "Next, we will try and find the optimal covariance function parameters, given that the inducing inputs are held in their current location. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["m.inducing_inputs.fix()\n", "m.optimize('bfgs')\n", "m.plot()\n", "print (m)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The poor location of the inducing inputs causes the model to 'underfit' the data. The length scale is much longer than for the full GP, and the noise variance is larger. This is because in this case the Kullback Leibler term in the free energy objective is dominating, and requires a larger length scale to improve the quality of the approximation. This is due to the poor location of the inducing inputs. \n", "\n", "### Optimizing Inducing Inputs\n", "\n", "Firstly we try optimzing the location of the inducing inputs to fix the problem, however we still get a larger length scale than the Gaussian process we sampled from (or the full GP fit we did at the beginning)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["m.randomize()\n", "m.Z.unconstrain()\n", "m.optimize('bfgs')\n", "m.plot()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The current set of inducing points provides partial coverage of the data space, but the fit remains suboptimal. To address this, we'll increase the number of inducing points \n", "\n", "### Train with More Inducing Points\n", "\n", "Now we try 12 inducing points, rather than the original 6 points. We then compare with the full Gaussian process likelihood."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Z = np.random.rand(12,1)*12\n", "m = GPy.models.SparseGPRegression(X,y,Z=Z)\n", "\n", "m.optimize('bfgs')\n", "m.plot()\n", "m_full.plot()\n", "print (m.log_likelihood(), m_full.log_likelihood())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This time, we've ensured an adequate number of inducing points, resulting in a fit closely resembling that of the GP . This is verified by the fact that the bound on the marginal likelihood is tight, which means that our variational approximation must be good (the difference between the bound and the true likelihood is the Kullback Leibler divergence between the approximation and the truth). "]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.13"}}, "nbformat": 4, "nbformat_minor": 4}